% Opcje klasy 'iithesis' opisane sa w komentarzach w pliku klasy. Za ich pomoca
% ustawia sie przede wszystkim jezyk i rodzaj (lic/inz/mgr) pracy, oraz czy na
% drugiej stronie pracy ma byc skladany wzor oswiadczenia o autorskim wykonaniu.
\documentclass[declaration,mgr,english,shortabstract]{iithesis}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}

%%%%% DANE DO STRONY TYTULOWEJ
% Niezaleznie od jezyka pracy wybranego w opcjach klasy, tytul i streszczenie
% pracy nalezy podac zarowno w jezyku polskim, jak i angielskim.
% Pamietaj o madrym (zgodnym z logicznym rozbiorem zdania oraz estetyka) recznym
% zlamaniu wierszy w temacie pracy, zwlaszcza tego w jezyku pracy. Uzyj do tego
% polecenia \fmlinebreak.
\englishtitle   {Formally verified algorithms and data structures in Coq: concepts and techniques}
\polishtitle    {Formalnie zweryfikowane algorytmy i struktury danych w Coqu: koncepcje i techniki}
\polishabstract {Omawiamy sposoby projektowania, implementowania, specyfikowania i weryfikowania funkcyjnych algorytmów i struktur danych, skupiając się bardziej na dowodach formalnych niż na asymptotycznej złożoności czy faktycznym czasie działania. Prezentujemy koncepcje i techniki, obie często opierające na jednej kluczowej zasadzie -- reifikacji i reprezentacji, za pomocą potężnego systemu typów Coqa, czegoś co w klasycznym, imperatywnym podejściu jest nieuchwytne, jak przepływ informacji w dowodzie czy kształt rekursji funkcji. Nasze podejście bogato ilustrujemy przykładami i studiami przypadku.}
\englishabstract{We discuss how to design, implement, specify and verify functional algorithms and data structures, concentrating on formal proofs rather than asymptotic complexity or actual performance. We present concepts and techniques, both of which often rely on one key principle -- the reification and representation, using Coq's powerful type system, of something which in the classical-imperative approach is intangible, like the flow of information in a proof or the shape of a function's recursion. We illustrate our approach using rich examples and case studies.}
% w pracach wielu autorow nazwiska mozna oddzielic poleceniem \and
\author         {Wojciech Kołowski}
% w przypadku kilku promotorow, lub koniecznosci podania ich afiliacji, linie
% w ponizszym poleceniu mozna zlamac poleceniem \fmlinebreak
\advisor        {dr Małgorzata Biernacka}
\date           {Czerwiec '20 chyba że koronawirus}                     % Data zlozenia pracy
% Dane do oswiadczenia o autorskim wykonaniu
%\transcriptnum {}                     % Numer indeksu
%\advisorgen    {dr. Jana Kowalskiego} % Nazwisko promotora w dopelniaczu
%%%%%

%%%%% WLASNE DODATKOWE PAKIETY
%
%\usepackage{graphicx,listings,amsmath,amssymb,amsthm,amsfonts,tikz}
\usepackage{amsfonts}
\usepackage{minted}
%
%%%%% WLASNE DEFINICJE I POLECENIA
%
%\theoremstyle{definition} \newtheorem{definition}{Definition}[chapter]
%\theoremstyle{remark} \newtheorem{remark}[definition]{Observation}
%\theoremstyle{plain} \newtheorem{theorem}[definition]{Theorem}
%\theoremstyle{plain} \newtheorem{lemma}[definition]{Lemma}
%\renewcommand \qedsymbol {\ensuremath{\square}}

\newcommand{\m}[1]{\texttt{#1}}

%\newcommand{\Empty}{\mathbf{0}}
%\newcommand{\Unit}{\mathbf{1}}
%\newcommand{\Bool}{\mathbf{2}}

% Thesis repository.
\newcommand{\repo}{\url{https://github.com/wkolowski/RandomCoqCode}}

% Some type theory markup.
\newcommand{\type}[2]{#1 \vdash #2}
\newcommand{\typeconv}[3]{#1 \vdash #2 \equiv #3}
\newcommand{\term}[3]{#1 \vdash #2 : #3}
\newcommand{\termconv}[4]{#1 \vdash #2 \equiv #3 : #4}

% Natural numbers.
\newcommand{\N}{\mathbb{N}}
\newcommand{\suc}[1]{\m{succ}(#1)}
\newcommand{\recN}[3]{\m{rec}_\N(#1, #2, #3)}

% Finite types.
\newcommand{\Fin}[1]{\m{Fin}(#1)}

%%%%%

\begin{document}

%%%%% POCZATEK ZASADNICZEGO TEKSTU PRACY

\chapter{Introduction} \label{ch1}

The title of this thesis is ``Formally verified functional algorithms and data structures in Coq: concepts and techniques''. In the Introduction, we take time to carefully explain what we mean by each of these phrases:

\begin{itemize}
    \item In \hyperref[paradigm]{section 1} we look at the social context that gives rise to an interesting misconception about ``algorithms and data structures''.
    \item In \hyperref[impfun]{section 2} we explain ``functional'' programming by comparing it with imperative programming.
    \item \hyperref[worlds]{Section 3} is a philosophical interlude in which we give an interesting interpretation of typical algorithmic activities as living in different worlds, with links between worlds being the source of potential errors.
    \item In \hyperref[formal]{section 4} we lay out the (almost) unity of the ``formally verified'' algorithmic world by contrasting it with the many worlds of the previous section.
    \item In \hyperref[mltt]{section 5} we describe Martin-L\"{o}f Type Theory, a formal system very close to the theoretical underpinnings of Coq, which can be seen as our model of computation.
    \item In \hyperref[coq]{section 6} we show how working ``in Coq'' looks like.
    \item In \hyperref[literature]{section 7} we do an ultra short literature review and discuss how our ``concepts and techniques'' differ from these that can be found there.
\end{itemize}

In the remaining chpaters we describe the actual concepts and techniques, using well-known and well-studied examples like quicksort, merge sort or binary heaps:

\begin{itemize}
    \item TODO
\end{itemize}

\section{The overarching paradigm} \label{paradigm}

The Free Dictionary says \cite{TheFreeDictionary} that an algorithm is

\begin{quote}
    A finite set of unambiguous instructions that, given some set of initial conditions, can be performed in a prescribed sequence to achieve a certain goal and that has a recognizable set of end conditions.
\end{quote}

The purpose of this entry is to explain the concept to a lay person, but it likely sounds just about right to the imperative programmer's ear too. To a functional ear, however, talking about sequences of instructions most certainly sounds as un-functional as it possibly could. It is no surprise then that some people wonder if it is even possible for algorithms to ``exist'' in a functional programming language, as exemplified by this StackOverflow question \cite{SO}. The poor soul asking this question had strongly associated algorithms with imperative languages in his head, even though functional languages have their roots in lambda calculus, a formal system invented precisely to capture what an algorithm is.

This situation is not uncommon and rather easy to explain. \textit{Imperative} algorithms and data structures \footnote{From now on when we write ``algorithms'' we will mean ``algorithms and data structures''} form one of the oldest, biggest, most widespread and prestigious fields of theoretical computer science. They are taught to every student in every computer science programme at every university. There's a huge amount of books and textbooks, with classics such as \cite{CLRS} \cite{TAOCP} known to pretty much everybody, at least by title. There's an even huger and still growing mass of research articles published in journals and conferences and I'm pretty sure there are at least some (imperative) algorithm researchers at every computer science department in existence.

But theoretical research and higher education are not the only strongholds of imperative algorithms. They are also pretty much synonymous with competitive programming, dominating most in-person and online computer science competitions like ICPC and HackerRank, respectively. They are seen as the thing that gifted high school students interested in computer science should pursue -- each time said students don't win medals in the International Olympiad in Informatics or the like, there will be some journalists complaining that their country's education system is ``falling behind''. In many countries, like Poland,\footnote{I believe the same is true for the rest of the former Eastern Bloc countries too and probably not much better in the West either.} if there's any high school level computer science education besides basic programming, it will be in imperative algorithms.

Imperative algorithms aren't just a mere field of study -- they are more of a mindset and a culture; following Kuhn's \cite{Kuhn} terminology, they can be said to form a paradigm. Because this paradigm is so immense, so powerful and so entrenched, we feel free to completely disregard it and devote ourselves and this thesis to studying a related field which did not yet reach the status of a paradigm -- functional algorithms -- focusing on proving their formal correctness.

But before we do that, we spend the rest of this chapter comparing the imperative and functional approaches to algorithms and briefly reviewing available literature on functional algorithms.

\section{Two flavours of algorithms} \label{impfun}

The differences between the fields imperative and functional algorithms are mostly a reflection of the differences between imperative and functional programming languages and only in a small part a matter of differences in research focus.

The basic data structure in imperative languages is the array, which abstracts a contiguous block of memory holding values of a particular type. More advanced data structures are usually records that hold values and pointers/references to other (or even the same) kinds of data structures. The basic control flow primitives for traversing these structures are various loops (\m{while}, \m{for}, \m{do} ... \m{while}) and branch/jump statements (\m{if}, \m{switch}, \m{goto}). The most important operation is assignment, which changes the value of a variable (and thus variables do actually vary, like the name suggests \cite{WordsMatter}). Computation is modeled by a series of operations which change the global state.

In functional languages the basic data structures are created using the mechanism of algebraic data types -- elements of each type so defined are trees whose node labels, branching and types of values held are specified by the user. The basic control flow primitives are pattern matching (checking the label and values of the tree's root) and recursion. The most important operation is function composition, which allows building complex functions from simpler ones. Computation is modeled by substitution of arguments for the formal parameters of a function. Variables don't actually vary -- they are just names for expressions. \cite{WordsMatter}

\begin{listing}[H]
    \begin{minted}{Java}
        int sum(int[] a)
        {
            int result = 0;
            for(int i = 0; i < a.length; ++i)
            {
                result += a[i];
            }
            return result;
        }
    \end{minted}
    \caption{A simple program for summing all integers stored in an array, written in an imperative pseudocode that resembles Java.}
\end{listing}

\begin{listing}[H]
    \begin{minted}{Haskell}
        data List a = Nil | Cons a (List a)

        sum : List Int -> Int
        sum Nil = 0
        sum (Cons x xs) = x + sum xs
    \end{minted}
    \begin{center}
        \caption{A simple program for summing all integers stored in a (singly-linked) list, written in a functional pseudocode that resembles Haskell.}
    \end{center}
\end{listing}

The two above programs showcase the relevant differences in practice. In both cases we want a function that sums integers stored in the most basic data structure. In the case of our pseudo-Java, this is a built-in array, whereas in our pseudo-Haskell, this is a list defined using the mechanism of algebraic data types, as something which is either empty (\m{Nil}) or something that has a head of type \m{a} and a tail, which is another list of values of type \m{a}.

In the imperative program, we declare a variable \m{result} to hold the current value of the sum and then we loop over the array. We start by creating an iterator variable \m{i} and setting it to \m{0}. We successively increment it with each iteration of the loop until it gets bigger than the length of the array and the loop finishes. At each iteration, we modify \m{result} by adding to it the array entry we're currently looking at.

In the functional program, we pattern match on the argument of the function. In case it is \m{Nil} (an empty list), we declare the result to be \m{0}. In case it is \m{Cons x xs} (a list with head \m{x} and tail \m{xs}), we declare that the result is computed by adding \m{x} and the recursively computed sum of numbers from the list \m{xs}.

Even though these programs are very simple, they depict the basic differences between imperative and functional algorithms quite well. Some less obvious differences are as follows.

First, functional data structures are by default immutable and thus persistent \cite{PersistentDataStructures}, whereas this is not the case for imperative data structures -- they have to be explicitly designed to support persistence. This means implementing some techniques, like backtracking, is very easy in functional languages, but it often requires much more effort in imperative languages. The price of persistence often is, however, increased memory usage.

Second, pointer juggling in imperative languages allows a more efficient implementation of some operations on tree-like structures than using algebraic data types, because nodes in such trees can have pointers not only to their children, but also to parents, siblings, etc. The most famous data structure whose functional, asymptotically optimal implementation is not known is union-find. \cite{FunctionalUnionFind}

The third point is that arrays, the bread-and-butter of imperative programming, provide random access read and write in $O(1)$ time, whereas equivalent functional random access structures work in $O(\log n)$ time where $n$ is the array size (or, at best, $O(\log i)$ where $i$ is the accessed index). This means that algorithms relying on constant time random access will suffer an asymptotic performance penalty when implemented in functional languages. \cite{Pippenger}

Even though this asymptotic penalty sounds gloomy, not all hope is lost, because of two reasons. First, mutable arrays can still be used in purely\footnote{``Pure'' and ``impure'' are loose terms used to classify functional languages. ``Pure'' roughly means that a language makes organized effort to separate programs that do ordinary number crunching or data processing from those that have side effects, like throwing exceptions or connecting to a database. An ``impure'' languages doesn't attempt at such a separation.} functional languages if the mutability is hidden behind a pure interface. An example of this is Haskell's ST monad. \cite{STMonad} Second, functional languages that are impure, like Standard ML or OCaml, allow using mutable arrays without much hassle, which often saves the day.

\section{The many-worlds interpretation of imperative algorithms} \label{worlds}

We stated earlier that we intend to concentrate on formally proving correctness of purely functional algorithms. Before doing that, we take a short detour to look at how it differs from the activities and workflows of the usual algorithmic business,\footnote{A person engaging in the usual algorithmic business we will call an \textit{algorithmist}.} what we embrace and what we're leaving out.

When an algorithmist encounters a problem, let's say ``How do I sort a list of integers?'', he will follow a path towards the solution which looks roughly like this:

\begin{itemize}
    \item Formulate a (more) precise specification of the problem.
    \item Design an algorithm that solves the problem and write it down using some kind of pseudocode, keeping a good balance between generality and detail.
    \item Prove that the algorithm is correct. If a proof is hard to find, this may be a sign that the algorithm is incorrect.
    \item Analyze complexity of the algorithm, i.e. how much resources (number of steps, bits memory, bits of randomness, etc.) does the algorithm need to solve the problem of a given size. If the algorithm needs too much resources, go back and try to design another one that needs less.
    \item Implement the algorithm and test whether the implementation is correct.
    \item Run some experiments to assess the actual performance of the implementation, preferably considering a few common scenarios: random data, data that often occurs in the real world, data constructed by an evil adversary, etc. If the performance is unsatisfying, try to find a better implementation or go back and design a better algorithm.
\end{itemize}

Of course this recipe is not stiff and some variations are possible. The two most popular ones would be:

\begin{itemize}
    \item The extremely practical, in which the specification and proof are dropped in favour of the test suite, the pseudocode is dropped in favour of the actual implementation, and the complexity analysis is only cursory and performed on the go during the design phase, in the algorithmist's head. This approach permeates comeptitive programming, because of the time pressure, and industry, because most ``algorithms'' there amount to unsophisticated data processing that doesn't need specification or proof.
    \item The extremely theoretical, in which there is no implementation and thus no tests and no performance assessment, and the most time-consuming part becomes the complexity analysis. This approach is widespread in teaching, where the implementation part is left to students as an exercise, and in theoretical research, where real-world applicability is not always the most important goal.
\end{itemize}

No matter the exact recipe, there is a very enlightening thing to be noticed, namely all the different worlds in which these activites take place. For example, the algorithm design process takes place in the algorithmist's mind, but after he's done, it is usually written down in some kind of comfortable pseudocode that allows skipping inessential detail. The actual implementation, in contrast, will be in some concrete programming language -- a very  popular one among algorithmists is \m{C++}.

The specification and the proof are usually written in English (or whatever the language of the algorithmist), intermingled with some mathematical symbols for numbers, relations, sets and quantifiers, but that's only the surface view. If we take a closer look, it will most likely turn out that the mathematical part is based on classical first-order logic\footnote{By ``classical'' we mean that the logic admits nonconstructive reasoning principles like proof by contradiction \cite{ProofByContradiction} -- its use can be seen, for example, in proofs of optimality.} and some kind of naive set theory. When pressed a bit, however, the algorithmist will readily assert that the set theory could be axiomatized using, let's say, the Zermelo-Fraenkel axioms.

The next hidden world, in which the complexity analysis takes its place, is the model of computation. In most cases it is not mentioned explicitly, just like logic and set theory in the previous paragraph, but usually it's easy to guess. Most algorithms are, after all, intended to be implemented on modern computers, and the model of computation most similar to the workings of real hardware is the RAM machine.

As we see, the typical solution of an algorithmic problem stems from a sequence (or rather, a loop) of activities which live in six different worlds: the world of abstract ideas, represented by the pseudocode, the world of programming, represented by the implementation language, the world of formal math, the world of informal math, the world of idealized execution, represented by the model of computation, and the world of concrete execution, represented by the hardware.

Even though the above many-worlds recipe and its variations work well in practice, as evidenced by the huge number of algorithms humanity put to use for solving its everyday problems, an inquisitive person interested in formal verification (or perhaps a philosopher) could ask a plethora of questions about how all these worlds fit together:

\begin{itemize}
    \item Does the implementation correspond to the pseudocode?
    \item Could the informally stated specification and proof really be cast into the claimed formal system?
    \item Does the model of computation actually model the hardware well?
    \item Could the model of computation (and the complexity analysis) be formalized?
    \item Assuming the implementation language is compiled, how is the source program related to machine code executed by the hardware, i.e. is compilation correct?\item Does the analysis agree with the implementation language semantics?\footnote{If it's the implementation that is analyzed and not the pseudocode.}
\end{itemize}

Each of these questions can be seen as pointing at a link between two worlds and each such link is a potential source of errors -- the worlds may not correspond too well. Because each pair of worlds can give rise to a potential mismatch, in theory there is a lot to worry about.

We allowed ourselves to wander into this lengthy overview of the usual algorithmic business in order to contrast it with the approach of formally verified functional algorithms, which is an attempt at getting rid of potential world mismatch errors by transfering (nearly) all of the activities into a single, unified, formal world.

\section{Formal verification as a world-collapser} \label{formal}

This formal world is Coq \cite{Coq}. Coq is a proof assistant -- a piece of software whose goal is helping to state and prove mathematical theorems. This help consists of providing a formal language
for doing so, called Gallina, a language for automating trivial proof steps and writing proof search procedures, called Ltac, and a languages of commands, called Vernacular, which simplifies tasks like looking up theorems from the standard library. The Coq ecosystem also has many useful tools, like CoqIDE, an IDE with good support for interactive proving.

Thanks to the Curry-Howard correspondence \cite{CurryHoward}, all of these great things can also be interpreted from the programmer's perspective. Gallina is a dependently typed functional programming language, Ltac is a language for automatic program synthesis, and Vernacular offers a suite of facilities similar to those found in most IDEs. CoqIDE can be seen as supporting interactive program construction (and interactively proving a program correct can be seen as a very powerful form of debugging!).

Applied to algorithms, Coq gives us numerous benefits. First, we no longer need to do pen-and-paper specifications and proofs, so the world of informal mathematics gets eliminated from the picture. Second, it has very powerful abstraction capabilities, which bypass the need for pseudocode -- we can directly implement the high-level idea behind the algorithm (an example of this will be provided in \hyperref[ch2]{Chapter 2}). This merges the worlds of ideas and programming into one. Third, as already mentioned, the Curry-Howard correspondence unifies functional programming and constructive mathematics, which further collapses the two already-collapsed worlds.

What we are left with are just three worlds instead of the six we had at the beginning: one for programming, proving and expressing high-level algorithmic ideas, one for the analysis (model of computation), and one for execution (hardware). Most of the links between activities now live in the first world and we can use the power of formal proofs to make sure there are no mismatches. We can prove that the algorithm satisfies the specification. If we decide to have more than one implementation (for example when we're looking for the most efficient one), we can prove they are equivalent. We can even prove that the specification is ``correct'', i.e. that the object it describes is unique. To sum up, we can avoid the various mismatches of previous section using formal proofs inside our unified world.

But as long as not all worlds were collapsed into one, there is still some room for error. First, becuase Coq is implemented in OCaml, there is no guarantee that our formal proofs are absolutely correct. In fact, as can be seen from \url{https://github.com/coq/coq/blob/master/dev/doc/critical-bugs}, Coq has experienced about one critical bug per year. This means if we want absolute certainty in our proofs, we need to verify Coq's implementation by hand, so the world of informal math is still somewhat alive.

The second point, related to the first, is that there is no guarantee the semantics of code executed by the hardware is the same as the semantics of the source code. There have been some attempts at formalizing various aspects of Coq in Coq \cite{CoqInCoq} \cite{CoqCoqCorrect} to this effect, but they have the status of exploratory research or work-in-progress. A more serious barrier that prevents us from ever fully formalizing Coq in Coq is G\"{o}del's incompleteness theorem.

Third, the model of computation still lives in a separate world, formally related neither to the code nor to the hardware. What's worse, there is a huge chasm between the preferred models of computation used in computational complexity (Turing machines \cite{Turing}) and algorithms (RAM machines) on the one hand, and the theory of programming languages (lambda calculi \cite{Church}) on the other hand -- see \cite{OtherTuringMachine} for a brief overview.

Regarding the third point, there has been some ongoing research whose goal is to bring lambda calculus on par with other models of computation, with interesting results like \cite{ReasonableMachine} showing that it's not as bad as complexity theorists think, but it's still very far from entering the mainstream consciousness. Another related line of research is implicit complexity theory -- an attempt to characterize complexity classes not in terms of resources needed by a machine to compute a function, but in terms of restricted, minimalistic programming languages that can be used to implement functions from these classes. See \cite{ICC1} \cite{ICC2} for an introduction. A third related body of research concerns cost semantics, a technique for specifying the resources needed by a programming language's constructs together with their operational behaviour, so that one no longer needs to consider the model of computation (or rather, the programming language becomes the model of computation). This opens up some interesting directions, like automatic complexity analyses during compilation time. See \cite{CostSemanticsBlog} for arguments why this is a viable alternative to machine-based computation models and \cite{CostSemantics} for an example paper using this technique.

We will not worry about the concerns raised, because a single formal world is still a huge win in terms of correctness. Regarding point 1, in practice, if Coq accepts a proof, then the proof is correct -- chances of running into a critical bug by accident are minimal. We're also not as much interested in running algorithms and analyzing their complexity, so points 2 and 3 don't matter to us at all.\footnote{This of course does not mean that we will study silly, exponentially slow algorithms...}

\section{Type theory (as a model of computation)} \label{mltt}

In this section we describe the basic principles on which Coq is founded. Coq is based on a formal system called Calculus of Constructions (CoC)\cite{CoC}, which later evolved into the Calculus of Inductive Constructions (CIC) \cite{CIC} and finally into Predicative Calculus of Cumulative Inductive Constructions (pCuIC) \cite{pCuIC}.

To avoid drowning in minute details, we instead describe Martin-L\"{o}f Type Theory (MLTT) \cite{MLTT1} \cite{MLTT2}, a closely related system which from now on we will call simply ``type theory''. In the next section, where we introduce Coq, we will point out the relevant differences from type theory as we go. The material presented in this section is standard, but in the light of previous section's closing paragraphs we encourage people familiar with type theory to read it as a description not of a formal system, but of a model of computation.

In type theory, as the name suggests, the basic objects of interest are types. In order to be meaningful, a type must first be formed. For example, we can always form the type $\N$ of natural numbers, but in general this is not the case: sometimes to form a type we must make an assumption or even several assumptions. We keep track of our assumptions using contexts. For example, if in a context $\Gamma$ we can form the types $A$ and $B$, written $\type{\Gamma}{A}$ and $\type{\Gamma}{B}$ respectively, then we can form the type of functions from $A$ to $B$, written $\type{\Gamma}{A \to B}$. The rules that govern type formation are called, unsurprisingly, formation rules.

After we have formed a type, we can manipulate its terms. For example, the type of natural numbers $\N$ has as terms, among others, $4$ and $2 + 2$, written $4 : \N$ and $2 + 2 : \N$, respectively. Similarly to the case of type formation, what is a term and what is not depends on the assumptions we have made: $n + m : \N$ only under the assumptions that $n : \N$ and $m : \N$, which we can formally write as $\term{n : \N, m : \N}{n + m}{\N}$. From now on we won't emphasize context-dependence, as everything we do in type theory depends on context.

Rules for creating terms are divided into two genres: introduction rules and elimination rules. Introduction rules for a type tell us how to create new terms of this type. For example, the introduction rules for $\N$ are $\term{\Gamma}{0}{\N}$, which says that $0$ is a natural number, and $\term{\Gamma, n : \N}{\suc{n}}{\N}$, which says that the successor of $n$ is a natural number given that $n$ is a natural number.

Elimination rules for a type, on the other hand, tell us how to use terms of this type to get terms of other types. For example, the simplest elimination rule for $\N$ says that if $\term{\Gamma}{z}{X}$ and $\term{\Gamma}{s}{\N \to X}$, then $\term{\Gamma, n : \N}{\recN{z}{s}{n}}{X}$. This rule, alternatively called a recursor, allows us to define functions by recursion on natural numbers, but to do proofs by induction we need a stronger elimination rule. We won't state it here to keep things simple.

The next thing we are interested in is convertibility, which formalizes the notion of computation. Intuitively, two terms are convertible if they evaluate to the same result. For example, we would expect that both $2 + 2$ and $4$ evaluate to $4$, and this is indeed the case, so they are convertible. To state that formally, we write $\termconv{\Gamma}{2 + 2}{4}{\N}$. Rules that govern the behaviour of convertibility, similarly to rules for term manipulation, come in two genres: computation rules and uniqueness rules.

Computation rules describe what happens when we first apply an introduction rule and then an elimination rule. For example, the first computation rule for $\N$ states that given $\term{\Gamma}{z}{X}$ and $\term{\Gamma}{s}{\N \to X}$, we have $\termconv{\Gamma}{\recN{z}{s}{0}}{z}{\N}$, and the second rule states that given $\term{\Gamma}{z}{X}$, $\term{\Gamma}{s}{\N \to X}$ and $\term{\Gamma}{n}{\N}$ we have $\termconv{\Gamma}{\recN{z}{s}{\suc{n}}}{s(\recN{z}{s}{n})}{\N}$.

Uniqueness rules, on the other hand, describe what happens when we first use an elimination rule and then an introduction rule. For example, the uniqueness rule for $\N$ states that given $\term{\Gamma}{n}{\N}$, we have $\termconv{\Gamma}{\recN{0}{\m{succ}}{n}}{n}{\N}$. Note, however, that this uniqueness rule for natural numbers is rarely included in most presentations of type theory.

The last thing to mention is how exactly contexts work and some of its consequences. First, there is the empty context, usually denoted by leaving empty space to the left of $\vdash$, for example $\type{}{\N}$ means that we can form the type $\N$ in the empty context. Second, if we have a context $\Gamma$ and in this context we can derive $\Gamma \vdash A$, then we can extend the context $\Gamma$ with a variable of this type, written $\Gamma, x : A$, provided that the name $x$ doesn't already occur in $\Gamma$.

The most important consequence of this is that we need a notion of convertibility for types. Because types are formed in contexts, and contexts can contain assumptions of the form $x : A$, types can depend on terms. For example, for any natural number $n$ we can form the type $\Fin{n}$ that has exactly $n$ elements, formally written $\type{\Gamma, n : \N}{\Fin{n}}$. This raises the question: is the type $\Fin{2 + 2}$ the same as the type $\Fin{4}$? The answer is yes -- because $2 + 2$ and $4$ are converible, these types are also convertible, formally written $\typeconv{\Gamma}{\Fin{2 + 2}}{\Fin{4}}$. The rules that govern type convertibility aren't very interesting -- they say that convertibility is an equivalence relation and that dependent types are convertible whenever the terms they depend on are convertible.

An astute reader has probably noticed that we sneaked into the above description a word that is familiar but was not defined, namely ``element''. The elements of a type $A$ are its closed terms that are in normal form. A term is closed if its context is empty and it is in normal form if it is the final result of evaluation,\footnote{We leave the idea of evaluation informal and won't describe it in more detail.} i.e. it can't be evaluated any further.

For example, $\term{}{4}{\N}$ is a closed term in normal form, $\term{}{2 + 2}{\N}$ is a closed term that is not in normal form and $\term{n : \N}{\suc{n}}{\N}$ is a term in normal form, but it is not closed because it depends on the assumption $n : \N$ (such terms are called open). It turns out that for natural numbers, the elements are precisely $0, \suc{0}, \suc{\suc{0}}, \suc{\suc{\suc{0}}}, \dots$ which we can interpret as $0, 1, 2, 3, \dots$ respectively. So, the elements of the type of natural numbers are precisely the natural numbers. Who would have guessed!

That's it. The basic idea behind type theory is very simple. Of course the above presentation is far from being complete -- to be, we would have to list all the formation, introduction, elimination, computation and uniqueness rules for all types, and also dozens of boring technical rules whose role is to make sure everything works as expected.

The explanation of elements prompts us to change our perspective on type theory from a model of computation back to foundational and pose the following question: what is the relation between type theory and set theory? After all, types have elements and sets have elements too. Below, we provide a short comparison.

Set theory has two basic kinds of objects -- propositions and sets -- living in two separate layers. Propositions live in the logical layer, which consists of first order classical logic, whereas sets live in the set-theoretical layer, which consists of set theory axioms. Type theory, on the other hand, has only one basic kind of objects, namely types, and only one layer -- the type layer. Both propositions and sets can in type theory be interpreted using types.

In set theory, the membership predicate $x \in A$ is a proposition which can be proved or disproved. It is decidable internally, because of the law of excluded middle, but not externally, because there is no computer program that can tell us whether $x \in A$. In type theory, $x : A$ is not a proposition, but a judgement, which means one can establish that it holds, but it makes no sense to negate it. It neither makes sense to talk about it's internal decidability. However, it is externally decidable, which means there is a computer program that checks whether it holds.

In set theory, sets are semantic entities that need not, in general, be disjoint, so an element can belong to many sets. In type theory, types are syntactic entities that are always disjoint. Elements can belong to only one type, or, to be more precise, if an element belongs to two types, then these types are convertible.

\section{Way of the Coq} \label{coq}

In this section we give a very short Coq tutorial. Before reading it, the unfamiliar reader should install CoqIDE\footnote{Available from \cite{Coq}.} and run the listing below\footnote{It can be found in the thesis' repository \repo in the directory \m{Thesis/Snippets/}} in interactive mode. This experience will greatly enrich the explanation.

\definecolor{CoqIDE}{RGB}{255,248,220}

\inputminted
[
    frame=lines,
    bgcolor=CoqIDE,
    linenos
]
{Coq}{Snippets/SpecVerification.v}

We start by importing modules for working with lists -- we will need some list functions and notations (Coq allows defining custom notations using a built-in notation mechanism), and also \m{lia}, a procedure for reasoning in \textbf{l}inear \textbf{i}nteger \textbf{a}rithmetic (hence the name).


The command \m{Print} is part of the Vernacular -- a language of useful commands for looking up definitions and theorems, and other things usually provided at the IDE level. We use it to recall the definition of \m{list} and \m{app} (we modified the outputs of these commands to simplify our explanations).

\m{list} is a parameterized family of inductive types. It means that, for each type \m{A}, there is a type list \m{list A} defined by giving the possible ways to construct its elements. An element of this type can be either \m{nil}, which represents an empty list, or \m{cons h t} for some $\m{h} : \m{A}$ and $\m{t} : \m{list A}$, which represents a list with head \m{h} and tail \m{t}. This is very similar to the definition of lists in pseudo-Haskell that we saw in \hyperref[impfun]{section 2}.

\m{app} is a function that concatenates two lists. It takes as arguments as type \m{A} and two lists of elements of type \m{A}, named \m{l1} and \m{l2}, and it returns an element of type \m{list A} as result. It is defined by recursion, as indicated by the keyword \m{fix}, and its termination is guaranteed because this recursion is structural, as indicated by the annotation \m{struct l1}. The definition goes by pattern matching on the argument \m{l1}. if it is a \m{nil}, the result is \m{l2} and if it is $\m{h} :: \m{t}$, then the result is $\m{h} :: \m{app t l2}$. Here the double colon $\m{::}$ is a handy notation for the constructor \m{cons}.

In \hyperref[worlds]{section 3} we said that in a formal setting we can prove a specification correct. This is actually the first of our techniques and in the rest of the listing our task will be to demonstrate how it works.

The thing we want to specify is a function that reverses a list and the specification, named \m{rev\_spec}, starts on line 20. It takes as arguments a type \m{A} and a function $\m{f} : \m{list A -> list A}$. The first argument is implicit, marked by curly braces around it instead of the usual parentheses, which means that we won't need to write it explicitly, because Coq can infer it on its own. So, to sum it up, we want \m{rev\_spec f} to mean ``\m{f} is a function that reverses lists''.

The definition says that \m{rev\_spec f} is a proposition\footnote{This is actually one of the differences between type theory as presented in the last section and Coq. In type theory, we represent propositions using ordinary types. In Coq, only types of sort \m{Prop} are propositions, whereas types of sort \m{Type} are not.} and that it is defined to be a record consisting of two fields. The first one is named \m{f\_app} and intuitively means that \m{f} anticommutes with list concatenation -- if we concatenate two lists and then reverse the result using \m{f}, it's the same as if we first reversed the lists and then concatenated them in reverse order. The second field is named \m{f\_singl} and means that on lists with only one element \m{f} acts like an identity function.

Now, let's take a break from Coq and make a short conceptual trip: how can we prove that a specification specifies exactly what we wanted? Of course, in general, we can't, because what we really want lives in the world of ideas, whereas the specification lives in the formal world. But if we are sure that the desired solution will meet the specification, then the best sanity check we can perform is to prove that the specification determines a unique object.
In such a situation if we are dissatisfied with the solution, we are sure that it is not because of any kind of technical error, like a bug, but simply because we were unable to express what we wanted.







\section{An ultra short literature review} \label{literature}

\chapter{A quick \textit{tour de} sort} \label{ch2}

\chapter{A man, a plan, a canal -- MSc thesis}

\section{Design}
\begin{itemize}
    \item First step: specification (describe the path from intuition to formal spec).
    \item Second step: design (describe the path from the concrete to the abstract by tracing a stub proof of correctness).
    \item We shouldn't require proofs in order to run programs. Clou: packed vs unpacked classes/records/modules.
\end{itemize}

\section{Techniques}
\begin{itemize}
    \item General recursion: Bove-Capretta method as the way to go.
    \item Functional induction as the way-to-go proof technique. Mention the Equations plugin.
\end{itemize}

\section{Topics}
\begin{itemize}
    \item Quicksort: in functional languages we have so powerful abstractions that we can actually implement \*algorithms\* and not just programs.
    \item  Braun mergesort: in order not to waste resources, we sometimes have to reify abstract patterns, like the splitting in mergesort.
    \item Binary heaps: a case study to show the basic workflow and that it's not that obvious how to get basic stuff right.
    \item Cool data structures: ternary search trees, finger trees.
\end{itemize}

\chapter{Conclusion}

Mention thesis' repository: \url{wkolowski.github.io/RandomCoqCode/Thesis/}

%%%%% BIBLIOGRAFIA

\begin{thebibliography}{1}

\bibitem{TheFreeDictionary}
    \url{https://www.thefreedictionary.com/algorithm}

\bibitem{SO}
    \textit{Do ``algorithms'' exist in Functional Programming?}, \\
    \url{https://stackoverflow.com/questions/25940327/do-algorithms-exist-in-functional-programming}

\bibitem{CLRS}
    Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein, \\
    \textit{Introduction to Algorithms}, \\
    \url{http://ressources.unisciel.fr/algoprog/s00aaroot/aa00module1/res/%5BCormen-AL2011%5DIntroduction_To_Algorithms-A3.pdf}

\bibitem{TAOCP}
    Donald Knuth, \textit{The Art of Computer Programming}

\bibitem{Kuhn}
    Thomas S. Kuhn, \textit{The Structure of Scientific Revolutions}

\bibitem{WordsMatter}
    Robert Harper, \textit{Words Matter}, 2012 \\
    \url{https://existentialtype.wordpress.com/2012/02/01/words-matter/}

\bibitem{PersistentDataStructures}
    Driscoll JR, Sarnak N, Sleator DD, Tarjan RE \\
    \textit{Making data structures persistent}, 1986

\bibitem{FunctionalUnionFind}
    Sylvain Conchon, Jean-Christophe Fillâtre, \\
    \textit{A Persistent Union-Find Data Structure}, 2007 \\
    \url{https://www.lri.fr/~filliatr/ftp/publis/puf-wml07.pdf}

\bibitem{Pippenger}
    Nicholas Pippenger, \textit{Pure versus impure Lisp}, 1996 \\
    \url{https://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf}

\bibitem{STMonad}
    John Launchbury, Simon Peyton Jones, \\
    \textit{Lazy Functional State Threads}, 1994 \\
    \url{https://www.microsoft.com/en-us/research/wp-content/uploads/1994/06/lazy-functional-state-threads.pdf}

\bibitem{ProofByContradiction}
    Andrej Bauer, \textit{Proof of negation and proof by contradiction}, 2010 \\
    \url{http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/}

\bibitem{Coq}
    \url{https://coq.inria.fr/}

\bibitem{CurryHoward}
    Morten Heine Sørensen, Paweł Urzyczyn, \\
    \textit{Lectures on the Curry-Howard Isomorphism}, 2006, \\
    \url{http://disi.unitn.it/~bernardi/RSISE11/Papers/curry-howard.pdf}

\bibitem{CoqInCoq}
    Bruno Barras, Benjamin Werner, \textit{Coq in Coq}, 1997, \\
    \url{http://www.lix.polytechnique.fr/Labo/Bruno.Barras/publi/coqincoq.pdf}

\bibitem{CoqCoqCorrect}
    Matthieu Sozeau, Simon Boulier, Yannick Forster, Nicolas Tabareau, Théo Winterhalter, \\
    \textit{Coq Coq Correct! Verification of Type Checking and Erasure for Coq, in Coq}, 2020, \\
    \url{https://www.irif.fr/~sozeau/research/publications/drafts/Coq_Coq_Correct.pdf}

\bibitem{Turing}
    Alan Turing, \textit{On Computable Numbers, with an Application to the Entscheidungsproblem}, 1936 \\
    \url{https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf}

\bibitem{Church}
    Alonzo Church, \textit{An Unsolvable Problem of Elementary Number Theory}, 1936 \\
    \url{https://www.ics.uci.edu/~lopes/teaching/inf212W12/readings/church.pdf}

\bibitem{OtherTuringMachine}
    Guy Blelloch, Robert Harper, \\
    \textit{$\lambda$-Calculus: The Other Turing Machine}, 2015 \\
    \url{https://www.cs.cmu.edu/~rwh/papers/lctotm/cs50.pdf}

\bibitem{ReasonableMachine}
    Ugo Dal Lago, Simone Martini, \\
    \textit{The Weak Lambda Calculus
    as a Reasonable Machine}, 2008 \\
    \url{https://www.di.unito.it/~deligu/CDR60_TCS/Martini.pdf}

\bibitem{ICC1}
    Ugo Dal Lago, \\
    \textit{A Short Introduction
    to Implicit Computational Complexity}, 2010 \\
    \url{http://cs.unibo.it/~dallago/FICQRA/esslli.pdf}

\bibitem{ICC2}
    Ugo Dal Lago,
    \textit{Machine-Free Complexity},
    2019 \\
    \url{https://caleidoscope.sciencesconf.org/data/DalLago_caleidoscopeslides.pdf}

\bibitem{CostSemanticsBlog}
    Robert Harper, \textit{Languages and Machines}, 2015 \\
    \url{https://existentialtype.wordpress.com/2011/03/16/languages-and-machines/}

\bibitem{CostSemantics}
    Norman Danner, Daniel R. Licata, Ramyaa Ramyaa \\
    \textit{Denotational Cost Semantics for
    Functional Languages with Inductive Types} \\
    \url{https://dlicata.wescreates.wesleyan.edu/pubs/dlr15inductive/dlr15inductive.pdf}

\bibitem{CoC}
    Thierry Coquand, Gérard Huet, \\
    \textit{The calculus of constructions}, 1984, \\
    \url{https://www.sciencedirect.com/science/article/pii/0890540188900053}

\bibitem{CIC}
    Christine Paulin-Mohring, \\
    \textit{Introduction to the Calculus of Inductive Constructions}, 2015 \\
    \url{https://hal.inria.fr/hal-01094195/document}

\bibitem{pCuIC}
    Amin Timany, Matthieu Sozeau, \\
    \textit{Consistency of the Predicative Calculus of Cumulative
    Inductive Constructions (pCuIC)}, 2018 \\
    \url{https://hal.inria.fr/hal-01615123v2/document}

\bibitem{MLTT1}
    Per Martin-L\"{o}f, \\
    \textit{An intuitionistic theory of types}, 1972 \\
    \url{https://archive-pml.github.io/martin-lof/pdfs/An-Intuitionistic-Theory-of-Types-1972.pdf}

\bibitem{MLTT2}
    Per Martin-L\"{o}f, \\
    \textit{Intuitionistic Type Theory}, 1984 \\
    \url{https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf}
    
\bibitem{SoftwareFoundations}
    Benjamin C. Pierce, Andrew W. Appel et al., \\
    \textit{Software Foundations}, 2019, \\
    \url{https://softwarefoundations.cis.upenn.edu/}

\bibitem{CoqArt}
    Yves Bertot and Pierre Castéran, \\
    \textit{Interactive Theorem Proving and Program Development \\ Coq'Art: The Calculus of Inductive Constructions}, 2004, \\
    \url{https://www.labri.fr/perso/casteran/CoqArt/}

\bibitem{CPDT}
    Adam Chlipala,
    \textit{Certified Programming with Dependent Types}, \\
    \url{http://adam.chlipala.net/cpdt/}

\end{thebibliography}

\end{document}