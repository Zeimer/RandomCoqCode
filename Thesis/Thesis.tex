% Opcje klasy 'iithesis' opisane sa w komentarzach w pliku klasy. Za ich pomoca
% ustawia sie przede wszystkim jezyk i rodzaj (lic/inz/mgr) pracy, oraz czy na
% drugiej stronie pracy ma byc skladany wzor oswiadczenia o autorskim wykonaniu.
\documentclass[declaration,mgr,english,shortabstract]{iithesis}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}

%%%%% DANE DO STRONY TYTULOWEJ
% Niezaleznie od jezyka pracy wybranego w opcjach klasy, tytul i streszczenie
% pracy nalezy podac zarowno w jezyku polskim, jak i angielskim.
% Pamietaj o madrym (zgodnym z logicznym rozbiorem zdania oraz estetyka) recznym
% zlamaniu wierszy w temacie pracy, zwlaszcza tego w jezyku pracy. Uzyj do tego
% polecenia \fmlinebreak.
\englishtitle   {Formally verified algorithms and data structures in Coq: concepts and techniques}
\polishtitle    {Formalnie zweryfikowane algorytmy i struktury danych w Coqu: koncepcje i techniki}
\polishabstract {Omawiamy sposoby projektowania, implementowania, specyfikowania i weryfikowania funkcyjnych algorytmów i struktur danych, skupiając się bardziej na dowodach formalnych niż na asymptotycznej złożoności czy faktycznym czasie działania. Prezentujemy koncepcje i techniki, obie często opierające na jednej kluczowej zasadzie -- reifikacji i reprezentacji, za pomocą potężnego systemu typów Coqa, czegoś co w klasycznym, imperatywnym podejściu jest nieuchwytne, jak przepływ informacji w dowodzie czy kształt rekursji funkcji. Nasze podejście bogato ilustrujemy przykładami i studiami przypadku.}
\englishabstract{We discuss how to design, implement, specify and verify functional algorithms and data structures, concentrating on formal proofs rather than asymptotic complexity or actual performance. We present concepts and techniques, both of which often rely on one key principle -- the reification and representation, using Coq's powerful type system, of something which in the classical-imperative approach is intangible, like the flow of information in a proof or the shape of a function's recursion. We illustrate our approach using rich examples and case studies.}
% w pracach wielu autorow nazwiska mozna oddzielic poleceniem \and
\author         {Wojciech Kołowski}
\usepackage{ulem} % TODO
% w przypadku kilku promotorow, lub koniecznosci podania ich afiliacji, linie
% w ponizszym poleceniu mozna zlamac poleceniem \fmlinebreak
\advisor        {dr Małgorzata Biernacka}
\date           {\sout{Czerwiec '20 chyba że koronawirus} \\ \sout{Jednak raczej wrzesień} \\ Z września też nici, ale spoko} % Data zlozenia pracy
% Dane do oswiadczenia o autorskim wykonaniu
%\transcriptnum {}                     % Numer indeksu
%\advisorgen    {dr. Jana Kowalskiego} % Nazwisko promotora w dopelniaczu
%%%%%

%%%%% WLASNE DODATKOWE PAKIETY
%
%\usepackage{graphicx,listings,amsmath,amssymb,amsthm,amsfonts,tikz}
\usepackage{amsfonts}
\usepackage{minted}
%
%%%%% WLASNE DEFINICJE I POLECENIA
%
%\theoremstyle{definition} \newtheorem{definition}{Definition}[chapter]
%\theoremstyle{remark} \newtheorem{remark}[definition]{Observation}
%\theoremstyle{plain} \newtheorem{theorem}[definition]{Theorem}
%\theoremstyle{plain} \newtheorem{lemma}[definition]{Lemma}
%\renewcommand \qedsymbol {\ensuremath{\square}}

\newcommand{\m}[1]{\texttt{#1}}

%\newcommand{\Empty}{\mathbf{0}}
%\newcommand{\Unit}{\mathbf{1}}
%\newcommand{\Bool}{\mathbf{2}}

% Thesis repository.
\newcommand{\repo}{\url{https://github.com/wkolowski/RandomCoqCode}}

% Some type theory markup.
\newcommand{\type}[2]{#1 \vdash #2}
\newcommand{\typeconv}[3]{#1 \vdash #2 \equiv #3}
\newcommand{\term}[3]{#1 \vdash #2 : #3}
\newcommand{\termconv}[4]{#1 \vdash #2 \equiv #3 : #4}

% Natural numbers.
\newcommand{\N}{\mathbb{N}}
\newcommand{\suc}[1]{\m{succ}(#1)}
\newcommand{\recN}[3]{\m{rec}_\N(#1, #2, #3)}

% Finite types.
\newcommand{\Fin}[1]{\m{Fin}(#1)}

% A counter for concepts and techniques.
\newcounter{cnt}
\newcommand{\runcnt}{\#\arabic{cnt}}
\newcommand{\concept}[1]
{
    \refstepcounter{cnt}
    \begin{center}
        \textbf{Formally verified algorithm concept/technique \runcnt} \\
        #1
    \end{center}
}

%%%%%

\begin{document}

%%%%% POCZATEK ZASADNICZEGO TEKSTU PRACY

\chapter{Introduction} \label{ch1}

The title of this thesis is ``Formally verified functional algorithms and data structures in Coq: concepts and techniques''. In the Introduction, we take time to carefully explain what we mean by each of these phrases:

\begin{itemize}
    \item In \hyperref[paradigm]{section 1} we look at the social context that gives rise to an interesting misconception about ``algorithms and data structures''.
    \item In \hyperref[impfun]{section 2} we explain ``functional'' programming by comparing it with imperative programming.
    \item \hyperref[worlds]{Section 3} is a philosophical interlude in which we give an interesting interpretation of typical algorithmic activities as living in different worlds, with links between worlds being the source of potential errors.
    \item In \hyperref[formal]{section 4} we lay out the (almost) unity of the ``formally verified'' algorithmic world by contrasting it with the many worlds of the previous section.
    \item In \hyperref[mltt]{section 5} we describe Martin-L\"{o}f Type Theory, a formal system very close to the theoretical underpinnings of Coq, which can be seen as our model of computation.
    \item In \hyperref[coq]{section 6} we show how working ``in Coq'' looks like.
    \item In \hyperref[literature]{section 7} we do an ultra short literature review and discuss how our ``concepts and techniques'' relate to what can be found there.
    \item In \hyperref[outline]{section 8} we outline the thesis structure.
\end{itemize}

\section{The overarching paradigm} \label{paradigm}

The Free Dictionary says \cite{TheFreeDictionary} that an algorithm is

\begin{quote}
    A finite set of unambiguous instructions that, given some set of initial conditions, can be performed in a prescribed sequence to achieve a certain goal and that has a recognizable set of end conditions.
\end{quote}

The purpose of this entry is to explain the concept to a lay person, but it likely sounds just about right to the imperative programmer's ear too. To a functional ear, however, talking about sequences of instructions most certainly sounds as un-functional as it possibly could. It is no surprise then that some people wonder if it is even possible for algorithms to ``exist'' in a functional programming language, as exemplified by this StackOverflow question \cite{SO}. The poor soul asking this question had strongly associated algorithms with imperative languages in his head, even though functional languages have their roots in lambda calculus, a formal system invented precisely to capture what an algorithm is.

This situation is not uncommon and rather easy to explain. \textit{Imperative} algorithms and data structures \footnote{From now on when we write ``algorithms'' we will mean ``algorithms and data structures''} form one of the oldest, biggest, most widespread and prestigious fields of theoretical computer science. They are taught to every student in every computer science programme at every university. There's a huge amount of books and textbooks, with classics such as \cite{CLRS} \cite{TAOCP} known to pretty much everybody, at least by title. There's an even huger and still growing mass of research articles published in journals and conferences and I'm pretty sure there are at least some (imperative) algorithm researchers at every computer science department in existence.

But theoretical research and higher education are not the only strongholds of imperative algorithms. They are also pretty much synonymous with competitive programming, dominating most in-person and online computer science competitions like ICPC and HackerRank, respectively. They are seen as the thing that gifted high school students interested in computer science should pursue -- each time said students don't win medals in the International Olympiad in Informatics or the like, there will be some journalists complaining that their country's education system is ``falling behind''. In many countries, like Poland,\footnote{I believe the same is true for the rest of the former Eastern Bloc countries too and probably not much better in the West either.} if there's any high school level computer science education besides basic programming, it will be in imperative algorithms.

Imperative algorithms aren't just a mere field of study -- they are more of a mindset and a culture; following Kuhn's \cite{Kuhn} terminology, they can be said to form a paradigm. Because this paradigm is so immense, so powerful and so entrenched, we feel free to completely disregard it and devote ourselves and this thesis to studying a related field which did not yet reach the status of a paradigm -- functional algorithms -- focusing on proving their formal correctness.

But before we do that, we spend the rest of this chapter comparing the imperative and functional approaches to algorithms and briefly reviewing available literature on functional algorithms.

\section{Two flavours of algorithms} \label{impfun}

The differences between the fields imperative and functional algorithms are mostly a reflection of the differences between imperative and functional programming languages and only in a small part a matter of differences in research focus.

The basic data structure in imperative languages is the array, which abstracts a contiguous block of memory holding values of a particular type. More advanced data structures are usually records that hold values and pointers/references to other (or even the same) kinds of data structures. The basic control flow primitives for traversing these structures are various loops (\m{while}, \m{for}, \m{do} ... \m{while}) and branch/jump statements (\m{if}, \m{switch}, \m{goto}). The most important operation is assignment, which changes the value of a variable (and thus variables do actually vary, like the name suggests \cite{WordsMatter}). Computation is modeled by a series of operations which change the global state.

In functional languages the basic data structures are created using the mechanism of algebraic data types -- elements of each type so defined are trees whose node labels, branching and types of values held are specified by the user. The basic control flow primitives are pattern matching (checking the label and values of the tree's root) and recursion. The most important operation is function composition, which allows building complex functions from simpler ones. Computation is modeled by substitution of arguments for the formal parameters of a function. Variables don't actually vary -- they are just names for expressions. \cite{WordsMatter}

\begin{listing}[H]
    \begin{minted}{Java}
        int sum(int[] a)
        {
            int result = 0;
            for(int i = 0; i < a.length; ++i)
            {
                result += a[i];
            }
            return result;
        }
    \end{minted}
    \caption{A simple program for summing all integers stored in an array, written in an imperative pseudocode that resembles Java.}
\end{listing}

\begin{listing}[H]
    \begin{minted}{Haskell}
        data List a = Nil | Cons a (List a)

        sum : List Int -> Int
        sum Nil = 0
        sum (Cons x xs) = x + sum xs
    \end{minted}
    \begin{center}
        \caption{A simple program for summing all integers stored in a (singly-linked) list, written in a functional pseudocode that resembles Haskell.}
    \end{center}
\end{listing}

The two above programs showcase the relevant differences in practice. In both cases we want a function that sums integers stored in the most basic data structure. In the case of our pseudo-Java, this is a built-in array, whereas in our pseudo-Haskell, this is a list defined using the mechanism of algebraic data types, as something which is either empty (\m{Nil}) or something that has a head of type \m{a} and a tail, which is another list of values of type \m{a}.

In the imperative program, we declare a variable \m{result} to hold the current value of the sum and then we loop over the array. We start by creating an iterator variable \m{i} and setting it to \m{0}. We successively increment it with each iteration of the loop until it gets bigger than the length of the array and the loop finishes. At each iteration, we modify \m{result} by adding to it the array entry we're currently looking at.

In the functional program, we pattern match on the argument of the function. In case it is \m{Nil} (an empty list), we declare the result to be \m{0}. In case it is \m{Cons x xs} (a list with head \m{x} and tail \m{xs}), we declare that the result is computed by adding \m{x} and the recursively computed sum of numbers from the list \m{xs}.

Even though these programs are very simple, they depict the basic differences between imperative and functional algorithms quite well. Some less obvious differences are as follows.

First, functional data structures are by default immutable and thus persistent \cite{PersistentDataStructures}, whereas this is not the case for imperative data structures -- they have to be explicitly designed to support persistence. This means implementing some techniques, like backtracking, is very easy in functional languages, but it often requires much more effort in imperative languages. The price of persistence often is, however, increased memory usage.

Second, pointer juggling in imperative languages allows a more efficient implementation of some operations on tree-like structures than using algebraic data types, because nodes in such trees can have pointers not only to their children, but also to parents, siblings, etc. The most famous data structure whose functional, asymptotically optimal implementation is not known is union-find. \cite{FunctionalUnionFind}

The third point is that arrays, the bread-and-butter of imperative programming, provide random access read and write in $O(1)$ time, whereas equivalent functional random access structures work in $O(\log n)$ time where $n$ is the array size (or, at best, $O(\log i)$ where $i$ is the accessed index). This means that algorithms relying on constant time random access will suffer an asymptotic performance penalty when implemented in functional languages. \cite{Pippenger}

Even though this asymptotic penalty sounds gloomy, not all hope is lost, because of two reasons. First, mutable arrays can still be used in purely\footnote{``Pure'' and ``impure'' are loose terms used to classify functional languages. ``Pure'' roughly means that a language makes organized effort to separate programs that do ordinary number crunching or data processing from those that have side effects, like throwing exceptions or connecting to a database. An ``impure'' languages doesn't attempt at such a separation.} functional languages if the mutability is hidden behind a pure interface. An example of this is Haskell's ST monad. \cite{STMonad} Second, functional languages that are impure, like Standard ML or OCaml, allow using mutable arrays without much hassle, which often saves the day.

\section{The many-worlds interpretation of imperative algorithms} \label{worlds}

We stated earlier that we intend to concentrate on formally proving correctness of purely functional algorithms. Before doing that, we take a short detour to look at how it differs from the activities and workflows of the usual algorithmic business,\footnote{A person engaging in the usual algorithmic business we will call an \textit{algorithmist}.} what we embrace and what we're leaving out.

When an algorithmist encounters a problem, let's say ``How do I sort a list of integers?'', he will follow a path towards the solution which looks roughly like this:

\begin{itemize}
    \item Formulate a (more) precise specification of the problem.
    \item Design an algorithm that solves the problem and write it down using some kind of pseudocode, keeping a good balance between generality and detail.
    \item Prove that the algorithm is correct. If a proof is hard to find, this may be a sign that the algorithm is incorrect.
    \item Analyze complexity of the algorithm, i.e. how much resources (number of steps, bits memory, bits of randomness, etc.) does the algorithm need to solve the problem of a given size. If the algorithm needs too much resources, go back and try to design another one that needs less.
    \item Implement the algorithm and test whether the implementation is correct.
    \item Run some experiments to assess the actual performance of the implementation, preferably considering a few common scenarios: random data, data that often occurs in the real world, data constructed by an evil adversary, etc. If the performance is unsatisfying, try to find a better implementation or go back and design a better algorithm.
\end{itemize}

Of course this recipe is not stiff and some variations are possible. The two most popular ones would be:

\begin{itemize}
    \item The extremely practical, in which the specification and proof are dropped in favour of the test suite, the pseudocode is dropped in favour of the actual implementation, and the complexity analysis is only cursory and performed on the go during the design phase, in the algorithmist's head. This approach permeates competitive programming, because of the time pressure, and industry, because most ``algorithms'' there amount to unsophisticated data processing that doesn't need specification or proof.
    \item The extremely theoretical, in which there is no implementation and thus no tests and no performance assessment, and the most time-consuming part becomes the complexity analysis. This approach is widespread in teaching, where the implementation part is left to students as an exercise, and in theoretical research, where real-world applicability is not always the most important goal.
\end{itemize}

No matter the exact recipe, there is a very enlightening thing to be noticed, namely all the different worlds in which these activities take place. For example, the algorithm design process takes place in the algorithmist's mind, but after he's done, it is usually written down in some kind of comfortable pseudocode that allows skipping inessential detail. The actual implementation, in contrast, will be in some concrete programming language -- a very popular one among algorithmists is \m{C++}.

The specification and the proof are usually written in English (or whatever the language of the algorithmist), intermingled with some mathematical symbols for numbers, relations, sets and quantifiers, but that's only the surface view. If we take a closer look, it will most likely turn out that the mathematical part is based on classical first-order logic\footnote{By ``classical'' we mean that the logic admits nonconstructive reasoning principles like proof by contradiction \cite{ProofByContradiction} -- its use can be seen, for example, in proofs of optimality.} and some kind of naive set theory. When pressed a bit, however, the algorithmist will readily assert that the set theory could be axiomatized using, let's say, the Zermelo-Fraenkel axioms.

The next hidden world, in which the complexity analysis takes its place, is the model of computation. In most cases it is not mentioned explicitly, just like logic and set theory in the previous paragraph, but usually it's easy to guess. Most algorithms are, after all, intended to be implemented on modern computers, and the model of computation most similar to the workings of real hardware is the RAM machine.

As we see, the typical solution of an algorithmic problem stems from a sequence (or rather, a loop) of activities which live in six different worlds: the world of abstract ideas, represented by the pseudocode, the world of programming, represented by the implementation language, the world of formal math, the world of informal math, the world of idealized execution, represented by the model of computation, and the world of concrete execution, represented by the hardware.

Even though the above many-worlds recipe and its variations work well in practice, as evidenced by the huge number of algorithms humanity put to use for solving its everyday problems, an inquisitive person interested in formal verification (or perhaps a philosopher) could ask a plethora of questions about how all these worlds fit together:

\begin{itemize}
    \item Does the implementation correspond to the pseudocode?
    \item Could the informally stated specification and proof really be cast into the claimed formal system?
    \item Does the model of computation actually model the hardware well?
    \item Could the model of computation (and the complexity analysis) be formalized?
    \item Assuming the implementation language is compiled, how is the source program related to machine code executed by the hardware, i.e. is compilation correct?\item Does the analysis agree with the implementation language semantics?\footnote{If it's the implementation that is analyzed and not the pseudocode.}
\end{itemize}

Each of these questions can be seen as pointing at a link between two worlds and each such link is a potential source of errors -- the worlds may not correspond too well. Because each pair of worlds can give rise to a potential mismatch, in theory there is a lot to worry about.

We allowed ourselves to wander into this lengthy overview of the usual algorithmic business in order to contrast it with the approach of formally verified functional algorithms, which is an attempt at getting rid of potential world mismatch errors by transferring (nearly) all of the activities into a single, unified, formal world.

\section{Formal verification as a world-collapser} \label{formal}

This formal world is Coq \cite{Coq}. Coq is a proof assistant -- a piece of software whose goal is helping to state and prove mathematical theorems. This help consists of providing a formal language
for doing so, called Gallina, a language for automating trivial proof steps and writing proof search procedures, called Ltac, and a languages of commands, called Vernacular, which simplifies tasks like looking up theorems from the standard library. The Coq ecosystem also has many useful tools, like CoqIDE, an IDE with good support for interactive proving.

Thanks to the Curry-Howard correspondence \cite{CurryHoward}, all of these great things can also be interpreted from the programmer's perspective. Gallina is a dependently typed functional programming language, Ltac is a language for automatic program synthesis, and Vernacular offers a suite of facilities similar to those found in most IDEs. CoqIDE can be seen as supporting interactive program construction (and interactively proving a program correct can be seen as a very powerful form of debugging!).

Applied to algorithms, Coq gives us numerous benefits. First, we no longer need to do pen-and-paper specifications and proofs, so the world of informal mathematics gets eliminated from the picture. Second, it has very powerful abstraction capabilities, which bypass the need for pseudocode -- we can directly implement the high-level idea behind the algorithm (an example of this will be provided in \hyperref[ch2]{Chapter 2}). This merges the worlds of ideas and programming into one. Third, as already mentioned, the Curry-Howard correspondence unifies functional programming and constructive mathematics, which further collapses the two already-collapsed worlds.

What we are left with are just three worlds instead of the six we had at the beginning: one for programming, proving and expressing high-level algorithmic ideas, one for the analysis (model of computation), and one for execution (hardware). Most of the links between activities now live in the first world and we can use the power of formal proofs to make sure there are no mismatches. We can prove that the algorithm satisfies the specification. If we decide to have more than one implementation (for example when we're looking for the most efficient one), we can prove they are equivalent. We can even prove that the specification is ``correct'', i.e. that the object it describes is unique. To sum up, we can avoid the various mismatches of previous section using formal proofs inside our unified world.

But as long as not all worlds were collapsed into one, there is still some room for error. First, because Coq is implemented in OCaml, there is no guarantee that our formal proofs are absolutely correct. In fact, as can be seen from \url{https://github.com/coq/coq/blob/master/dev/doc/critical-bugs}, Coq has experienced about one critical bug per year. This means if we want absolute certainty in our proofs, we need to verify Coq's implementation by hand, so the world of informal math is still somewhat alive.

The second point, related to the first, is that there is no guarantee the semantics of code executed by the hardware is the same as the semantics of the source code. There have been some attempts at formalizing various aspects of Coq in Coq \cite{CoqInCoq} \cite{CoqCoqCorrect} to this effect, but they have the status of exploratory research or work-in-progress. A more serious barrier that prevents us from ever fully formalizing Coq in Coq is G\"{o}del's incompleteness theorem.

Third, the model of computation still lives in a separate world, formally related neither to the code nor to the hardware. What's worse, there is a huge chasm between the preferred models of computation used in computational complexity (Turing machines \cite{Turing}) and algorithms (RAM machines) on the one hand, and the theory of programming languages (lambda calculi \cite{Church}) on the other hand -- see \cite{OtherTuringMachine} for a brief overview.

Regarding the third point, there has been some ongoing research whose goal is to bring lambda calculus on par with other models of computation, with interesting results like \cite{ReasonableMachine} showing that it's not as bad as complexity theorists think, but it's still very far from entering the mainstream consciousness. Another related line of research is implicit complexity theory -- an attempt to characterize complexity classes not in terms of resources needed by a machine to compute a function, but in terms of restricted, minimalistic programming languages that can be used to implement functions from these classes. See \cite{ICC1} \cite{ICC2} for an introduction. A third related body of research concerns cost semantics, a technique for specifying the resources needed by a programming language's constructs together with their operational behaviour, so that one no longer needs to consider the model of computation (or rather, the programming language becomes the model of computation). This opens up some interesting directions, like automatic complexity analyses during compilation time. See \cite{CostSemanticsBlog} for arguments why this is a viable alternative to machine-based computation models and \cite{CostSemantics} for an example paper using this technique.

We will not worry about the concerns raised, because a single formal world is still a huge win in terms of correctness. Regarding point 1, in practice, if Coq accepts a proof, then the proof is correct -- chances of running into a critical bug by accident are minimal. We're also not as much interested in running algorithms and analyzing their complexity, so points 2 and 3 don't matter to us at all.\footnote{This of course does not mean that we will study silly, exponentially slow algorithms...}

\section{Type theory (as a model of computation)} \label{mltt}

In this section we describe the basic principles on which Coq is founded. Coq is based on a formal system called Calculus of Constructions (CoC)\cite{CoC}, which later evolved into the Calculus of Inductive Constructions (CIC) \cite{CIC} and finally into Predicative Calculus of Cumulative Inductive Constructions (pCuIC) \cite{pCuIC}.

To avoid drowning in minute details, we instead describe Martin-L\"{o}f Type Theory (MLTT) \cite{MLTT1} \cite{MLTT2}, a closely related system which from now on we will call simply ``type theory''. In the next section, where we introduce Coq, we will point out the relevant differences from type theory as we go. The material presented in this section is standard, but in the light of previous section's closing paragraphs we encourage people familiar with type theory to read it as a description not of a formal system, but of a model of computation.

In type theory, as the name suggests, the basic objects of interest are types. In order to be meaningful, a type must first be formed. For example, we can always form the type $\N$ of natural numbers, but in general this is not the case: sometimes to form a type we must make an assumption or even several assumptions. We keep track of our assumptions using contexts. For example, if in a context $\Gamma$ we can form the types $A$ and $B$, written $\type{\Gamma}{A}$ and $\type{\Gamma}{B}$ respectively, then we can form the type of functions from $A$ to $B$, written $\type{\Gamma}{A \to B}$. The rules that govern type formation are called, unsurprisingly, formation rules.

After we have formed a type, we can manipulate its terms. For example, the type of natural numbers $\N$ has as terms, among others, $4$ and $2 + 2$, written $4 : \N$ and $2 + 2 : \N$, respectively. Similarly to the case of type formation, what is a term and what is not depends on the assumptions we have made: $n + m : \N$ only under the assumptions that $n : \N$ and $m : \N$, which we can formally write as $\term{n : \N, m : \N}{n + m}{\N}$. From now on we won't emphasize context-dependence, as everything we do in type theory depends on context.

Rules for creating terms are divided into two genres: introduction rules and elimination rules. Introduction rules for a type tell us how to create new terms of this type. For example, the introduction rules for $\N$ are $\term{\Gamma}{0}{\N}$, which says that $0$ is a natural number, and $\term{\Gamma, n : \N}{\suc{n}}{\N}$, which says that the successor of $n$ is a natural number given that $n$ is a natural number.

Elimination rules for a type, on the other hand, tell us how to use terms of this type to get terms of other types. For example, the simplest elimination rule for $\N$ says that if $\term{\Gamma}{z}{X}$ and $\term{\Gamma}{s}{\N \to X}$, then $\term{\Gamma, n : \N}{\recN{z}{s}{n}}{X}$. This rule, alternatively called a recursor, allows us to define functions by recursion on natural numbers, but to do proofs by induction we need a stronger elimination rule. We won't state it here to keep things simple.

The next thing we are interested in is convertibility, which formalizes the notion of computation. Intuitively, two terms are convertible if they evaluate to the same result. For example, we would expect that both $2 + 2$ and $4$ evaluate to $4$, and this is indeed the case, so they are convertible. To state that formally, we write $\termconv{\Gamma}{2 + 2}{4}{\N}$. Rules that govern the behaviour of convertibility, similarly to rules for term manipulation, come in two genres: computation rules and uniqueness rules.

Computation rules describe what happens when we first apply an introduction rule and then an elimination rule. For example, the first computation rule for $\N$ states that given $\term{\Gamma}{z}{X}$ and $\term{\Gamma}{s}{\N \to X}$, we have $\termconv{\Gamma}{\recN{z}{s}{0}}{z}{\N}$, and the second rule states that given $\term{\Gamma}{z}{X}$, $\term{\Gamma}{s}{\N \to X}$ and $\term{\Gamma}{n}{\N}$ we have $\termconv{\Gamma}{\recN{z}{s}{\suc{n}}}{s(\recN{z}{s}{n})}{\N}$.

Uniqueness rules, on the other hand, describe what happens when we first use an elimination rule and then an introduction rule. For example, the uniqueness rule for $\N$ states that given $\term{\Gamma}{n}{\N}$, we have $\termconv{\Gamma}{\recN{0}{\m{succ}}{n}}{n}{\N}$. Note, however, that this uniqueness rule for natural numbers is rarely included in most presentations of type theory.

The last thing to mention is how exactly contexts work and some of its consequences. First, there is the empty context, usually denoted by leaving empty space to the left of $\vdash$, for example $\type{}{\N}$ means that we can form the type $\N$ in the empty context. Second, if we have a context $\Gamma$ and in this context we can derive $\Gamma \vdash A$, then we can extend the context $\Gamma$ with a variable of this type, written $\Gamma, x : A$, provided that the name $x$ doesn't already occur in $\Gamma$.

The most important consequence of this is that we need a notion of convertibility for types. Because types are formed in contexts, and contexts can contain assumptions of the form $x : A$, types can depend on terms. For example, for any natural number $n$ we can form the type $\Fin{n}$ that has exactly $n$ elements, formally written $\type{\Gamma, n : \N}{\Fin{n}}$. This raises the question: is the type $\Fin{2 + 2}$ the same as the type $\Fin{4}$? The answer is yes -- because $2 + 2$ and $4$ are convertible, these types are also convertible, formally written $\typeconv{\Gamma}{\Fin{2 + 2}}{\Fin{4}}$. The rules that govern type convertibility aren't very interesting -- they say that convertibility is an equivalence relation and that dependent types are convertible whenever the terms they depend on are convertible.

An astute reader has probably noticed that we sneaked into the above description a word that is familiar but was not defined, namely ``element''. The elements of a type $A$ are its closed terms that are in normal form. A term is closed if its context is empty and it is in normal form if it is the final result of evaluation,\footnote{We leave the idea of evaluation informal and won't describe it in more detail.} i.e. it can't be evaluated any further.

For example, $\term{}{4}{\N}$ is a closed term in normal form, $\term{}{2 + 2}{\N}$ is a closed term that is not in normal form and $\term{n : \N}{\suc{n}}{\N}$ is a term in normal form, but it is not closed because it depends on the assumption $n : \N$ (such terms are called open). It turns out that for natural numbers, the elements are precisely $0, \suc{0}, \suc{\suc{0}}, \suc{\suc{\suc{0}}}, \dots$ which we can interpret as $0, 1, 2, 3, \dots$ respectively. So, the elements of the type of natural numbers are precisely the natural numbers. Who would have guessed!

That's it. The basic idea behind type theory is very simple. Of course the above presentation is far from being complete -- to be, we would have to list all the formation, introduction, elimination, computation and uniqueness rules for all types, and also dozens of boring technical rules whose role is to make sure everything works as expected.

The explanation of elements prompts us to change our perspective on type theory from a model of computation back to foundational and pose the following question: what is the relation between type theory and set theory? After all, types have elements and sets have elements too. Below, we provide a short comparison.

Set theory has two basic kinds of objects -- propositions and sets -- living in two separate layers. Propositions live in the logical layer, which consists of first order classical logic, whereas sets live in the set-theoretical layer, which consists of set theory axioms. Type theory, on the other hand, has only one basic kind of objects, namely types, and only one layer -- the type layer. Both propositions and sets can in type theory be interpreted using types.

% TODO: membetship predicate is propsition

In set theory, the membership predicate $x \in A$ is a proposition which can be proved or disproved. It is decidable internally, because of the law of excluded middle, but not externally, because there is no computer program that can tell us whether $x \in A$. In type theory, $x : A$ is not a proposition, but a judgment, which means one can establish that it holds, but it makes no sense to negate it. It neither makes sense to talk about it's internal decidability. However, it is externally decidable, which means there is a computer program that checks whether it holds.

In set theory, sets are semantic entities that need not, in general, be disjoint, so an element can belong to many sets. In type theory, types are syntactic entities that are always disjoint. Elements can belong to only one type, or, to be more precise, if an element belongs to two types, then these types are convertible.

\section{Way of the Coq} \label{coq}

In this section we give a very short Coq tutorial. Before reading it, the unfamiliar reader should install CoqIDE\footnote{Available from \cite{Coq}.} and run the listing below\footnote{It can be found in the thesis' repository \repo in the directory \m{Thesis/Snippets/}} in interactive mode. This experience will greatly enrich the explanation.

\definecolor{CoqIDE}{RGB}{255,248,220}

\inputminted
[
    frame=lines,
    bgcolor=CoqIDE,
    linenos
]
{Coq}{Snippets/SpecVerification.v}

We start by importing modules for working with lists -- we will need some list functions and notations (Coq allows defining custom notations using a built-in notation mechanism), and also \m{lia}, a procedure for reasoning in \textbf{l}inear \textbf{i}nteger \textbf{a}rithmetic (hence the name).


The command \m{Print} is part of the Vernacular -- a language of useful commands for looking up definitions and theorems, and other things usually provided at the IDE level. We use it to recall the definition of \m{list} and \m{app} (we modified the outputs of these commands to simplify our explanations).

\m{list} is a parameterized family of inductive types. This means that for each type \m{A}, there is a type list \m{list A}, defined by giving the possible ways to construct its elements. An element of this type can be either \m{nil}, which represents an empty list, or \m{cons h t} for some $\m{h} : \m{A}$ and $\m{t} : \m{list A}$, which represents a list with head \m{h} and tail \m{t}. This is very similar to the definition of lists in pseudo-Haskell that we saw in \hyperref[impfun]{section 2}.

\m{app} is a function that concatenates two lists. It takes as arguments a type \m{A} and two lists of elements of type \m{A}, named \m{l1} and \m{l2}, and it returns an element of type \m{list A} as result. The first argument is surrounded by curly braces, which means it is implicit -- we won't need to write it, because Coq can infer it on its own. The other arguments are surrounded by parentheses, which means they are explicit -- we need to provide them when applying the function.

\m{app} is defined by recursion, as indicated by the keyword \m{fix}, and its termination is guaranteed because this recursion is structural, as indicated by the annotation \m{struct l1}. The definition goes by pattern matching on the argument \m{l1}. if it is a \m{nil}, the result is \m{l2} and if it is $\m{h} :: \m{t}$, then the result is $\m{h} :: \m{app t l2}$. Here the double colon $\m{::}$ is a handy notation for the constructor \m{cons}.

In \hyperref[worlds]{section 3} we said that in a formal setting we can prove a specification correct. This is actually the first of our techniques and in the rest of the listing our task will be to demonstrate how it works. The thing we want to specify is a function for reversing lists and the specification, named \m{rev\_spec}, starts on line 20. It takes as arguments a type \m{A} and a function $\m{f} : \m{list A -> list A}$ and our intention is that \m{rev\_spec} \m{f} means ``\m{f} is a function that reverses lists''.

The definition says that \m{rev\_spec f} is a proposition\footnote{This is actually one of the differences between type theory as presented in the last section and Coq. In type theory, we represent propositions using ordinary types. In Coq, only types of sort \m{Prop} are propositions, whereas types of sort \m{Type} are not.} and that it is defined to be a record consisting of two fields. The first one is named \m{f\_app} and intuitively means that \m{f} anticommutes with list concatenation -- if we concatenate two lists and then reverse the result using \m{f}, it's the same as if we first reversed the lists and then concatenated them in the opposite order. The second field is named \m{f\_singl} and means that on lists with only one element \m{f} acts like an identity function.

Now, let's take a break from Coq and make a short conceptual trip: how can we prove that a specification specifies precisely what we wanted? Of course, in general, we can't, because what we really want lives in the world of ideas, whereas the specification lives in the formal world. But if we are sure that the desired solution will meet the specification, the best sanity check we can perform is proving that the specification determines a unique object.
In such a situation if we are dissatisfied with the solution, we are sure that it is not because of some bug or other kind of technical error, but simply because we were unable to express what we wanted.

Back to Coq, the theorem \m{rev\_spec\_unique} is the fulfillment of our plan of ``proving the specification correct''. Such a theorem in Coq has the same status as an ordinary definition -- the statement of the theorem is a type and to prove it we need to construct an element of that type. However, the means of construction are different from those typically used for definitions: we don't provide a proof term explicitly, like the body of \m{app} between \m{match} and \m{end} was provided explicitly, but instead we use Coq's tactic language called Ltac.

In Ltac, we can manipulate tactics, which can be seen as things that encapsulate a particular line of mathematical reasoning and argumentation (or, from the programmer's perspective, as techniques for synthesizing particular kinds of programs). The most basic tactics directly correspond to introduction and elimination rules of type theory as described in the previous section, and to basic features of Coq's term language, like pattern matching. More complex tactics can be created from simpler ones by using features provided by Ltac, like various tactic combinators, searching hypotheses and variables in the proof context (and more advanced features for context management), many flavours of backtracking and the ability to inspect Coq's terms as syntax.

The last of these is something that is not possible using the term language, as it would allow distinguishing between convertible terms and thus lead to contradiction. For example, given a \m{P} : \m{Prop}, we can't use the term language to check whether it's of the form \m{Q /\char`\\\ R}, but we can do that using Ltac. We can use this together with other Ltac features to, for example, implement a tactic that can decide whether a proposition is an intuitionistic tautology or not.

Back to the proof of our theorem, the idea is as follows. The proof is by induction on \m{l}. This splits the proof into two cases: in the first one \m{l} is \m{[]} and in the second one \m{l} is $\m{h} :: \m{t}$ for some head \m{h} and tail \m{t}. In the first case, we exploit the fact that \m{f [] = f [] ++ f []} (which stems from specializing one of the specification's clauses with \m{[]}) to argue that the length of \m{f []} must be zero, and thus that \m{f []} must equal \m{[]} (and analogously for \m{g}). In the second case, we make heavy use of equations coming from the specification and the induction hypotheses.

We won't explain how this proof idea is actually carried out, because such an explanation would be far less enlightening than just going over the proof script in CoqIDE. We will only gloss over the meaning of tactics that were used:

\begin{itemize}
    \item \m{intros} lets us assume hypotheses and move universally quantified variables from the goal into the context.
    \item \m{induction} starts a proof by induction, splitting the goal into as many subgoals as there are cases.
    \item \m{cbn} performs computations, like simplifying \m{2 + 2} to \m{4}.
    \item \m{specialize} instantiates universally quantified hypotheses with particular objects.
    \item \m{apply} implements modus ponens, allowing us to transform \m{P} into \m{Q} in the context, given a proof of \m{P -> Q}.
    \item \m{rewrite}, given a hypothesis that is an equation, replaces the occurrences of its left-hand side with its right-hand side in the goal or another hypothesis.
    \item \m{destruct} implements reasoning by cases.
    \item \m{reflexivity} allows us to conclude that \m{x = x}.
    \item \m{lia} is the powerful tactic for dealing with arithmetic mentioned earlier. In our case we use it to prove that from \m{n + n = n} it follows that \m{n = 0}.
    \item \m{change} allows us to replace a term or its part with a convertible term. For example, having computed \m{2 + 2} to \m{4}, we could use \m{change} to ``reverse'' this computation, changing \m{4} back into \m{2 + 2}.
\end{itemize}

Our short tutorial on Coq has come to an end. From now on, we assume the reader is familiar with the technical workings of Coq -- while discussing further code listings, we will only explain the concepts and techniques.\footnote{When numbering concepts and techniques, we won't distinguish between them, as any concept, to be useful, has to be concretely realized by some technique and behind any technique there is some concept.} The moral of our example can be summarized as follows.

\concept{Prove that the specification determines a unique object.}

\section{An ultra short literature review} \label{literature}

In this section we briefly discuss the not so large body of literature relevant to our work. In case the reader still doesn't feel comfortable with Coq after last section's tutorial, we start by listing some standard learning materials on the topic:

\begin{itemize}
    \item The first volume of Software Foundations \cite{SoftwareFoundations} is a textbook aimed at teaching Coq fundamentals to students familiar with neither functional programming nor formal proof. It is very beginner-friendly and well-suited for self study, but it doesn't cover all Coq-related topics exhaustively.
    \item Coq'Art \cite{CoqArt} is more of a reference work than a textbook -- it tries to be comprehensive, covering topics like proof by reflection which are missing from Software Foundations, but it is also a bit old, having been published in 2004. Watch out for the outdated treatment of coinduction!
    \item Certified Programming with Dependent Types \cite{CPDT} is a book aimed at more advanced Coq users. It focuses on programming with dependent types and automating proofs with powerful hand-crafted tactics, but also covers topics like general recursion and dealing with axioms.
\end{itemize}

As we can see, the book-length literature on Coq is quite scarce and sometimes dated. The situation in the field of functional algorithms and data structures is even worse. In fact, there is only one significant book in the whole field, namely Purely Functional Data Structures (PFDS for short) \cite{Okasaki}, which is an extended version of Chris Okasaki's PhD thesis \cite{OkasakiPhD} (see \cite{Okasaki10YearsLater} for the story behind the book). This book is basically a turning point which demarcates two periods in the field's history: pre-Okasaki and post-Okasaki. We won't describe these periods and their literature in detail, as a very good summary is available in this StackOverflow thread \cite{SinceOkasaki}.

Even though the book is great, it won't be of much inspiration for us. The reasons stem directly from the book's main themes. The first one is exploring the various algorithmic uses of lazy evaluation. Even though Coq makes lazy evaluation readily available, it does so using the tactic \m{cbn} and command \m{Eval cbn in t}, whereas the term language has barely any control over the evaluation order. This means that large swaths of the book are at odds with Coq.

The second theme in the book, somewhat related to the first, are techniques for analysis of amortized complexity of lazy data structures. Okasaki was actually the first person to realize that in the purely functional world, amortization is possible (before it was though that it is contradicted by persistence) and that it is inseparably tied to lazy evaluation. This aspect of PFSD won't be important for us because, as we have already underlined many times, we are not interested in complexity, but in formal correctness.

The third big theme of the book are numerical representations. It is the idea that functional data structures are analogous to various obscure positional number systems that aren't at all used for other purposes. In this view, inserting an element into a data structure is analogous to incrementing a number, deleting an element is analogous to decrementing a number, merging two data structures is analogous to adding numbers and so on. While it is a very interesting and worthwhile idea, its purpose is coming up with new data structures -- something we won't do here.

The other book that is relevant to our work is ``Verified Functional Algorithms'' \cite{VFA}, which is actually the third volume of the aforementioned Software Foundations series \cite{SoftwareFoundations}. It is in fact much more relevant than PFDS, because it focuses on proving algorithms correct using Coq, just as we do. In our opinion however, the approach presented in VFA has many shortcomings:

\begin{itemize}
    \item It presents the discussed algorithms in their final form without discussing how they were designed in the first place (or how they were translated from their imperative progenitors).
    \item It is not abstract enough -- all too often it considers only the case where the keys/values of a data structure are natural numbers and skips the general case, where they are of any type with decidable equality/partial order etc.
    \item It does not deal with techniques for defining general recursive functions properly. Even though it presents a method of defining general recursive functions, it doesn't discuss its principles or what to do if it fails. Often it skips the matter completely and just uses the so called ``fuel recursion'' which is not very hygienic.
\end{itemize}

Besides these shortcomings, there are also some other differences, which are more cosmetic in nature. First, VFA uses Coq's module system, whereas we will use the typeclass system. Second, even though just like us it is not that much concerned with performance, VFA sees the performance of an algorithm implemented in Coq as a property of the OCaml program extracted\footnote{Coq has a mechanism called extraction, which is a form of code generation that transforms Coq code into computationally equivalent code in OCaml, Haskell or Scheme.} from the Coq code, rather than of the Coq program itself, like we do.

\section{Outline of the thesis} \label{outline}

Having given a thorough introduction, it should now be clear what we mean by ``formally verified functional algorithms and data structures in Coq''. In the remaining chapters of the thesis we describe the actual concepts and techniques'', using well-known and well-studied examples like quicksort, merge sort or binary heaps. The rest of the thesis is structured as follows:

\begin{itemize}
    \item In \hyperref[quicksort]{Chapter 2} we look at quicksort inside and out, describing the bulk of our approach.
    \item In chapter 3, ...
    \item TODO: put here the remaining chapters after they're written.
\end{itemize}

\chapter{A quick \textit{tour de} sort} \label{quicksort}

The field of algorithms and data structures can be easily separated into two subfields: the algorithms and the data structures. These two go hand in hand: all algorithms operate on data structures, no matter how primitive, and data structures exist precisely in order for algorithms to do something with them.

We have to start somewhere, however, and so we choose to break this vicious circle at algorithms. In this chapter we will look at the problem of sorting a list -- one of the most basic data structures -- and at quicksort, one of the fastest and most elegant solutions of this problem.

Quicksort is probably also the most researched algorithm in the whole imperative paradigm, which makes it a perfect candidate to showcase our core concepts and techniques for specifying, implementing and verifying algorithms in the functional paradigm.

\section{Specify the problem} \label{specify}

\begin{center}
    \textbf{Formally verified algorithm concept/technique \#0} \\
    Find a specification of the problem that is both abstract and easy to use.
\end{center}

\begin{itemize}
    \item Definition of sorting for naturals using $<$ -- not easy to use.
    \item An inductive definition for naturals -- not abstract.
    \item Decidable linear orders -- abstract.
\end{itemize}

\section{Abstract the algorithm} \label{template}

\concept{Implement an abstract template that captures the idea of the algorithm. You can ignore the matter of termination at first.}

Hints about turning the termination checker off. Present an imperative version of quicksort and maybe the story about people not believing Hoare the implementation could work.

\section{Prove termination} \label{termination}

\concept{Implement a better template using a type that represents the shape of the algorithm's recursion. Prove termination using well-founded induction.}

Bove-Capretta method, at first crudely using sort Type, then maybe Prop but not necessarily.

\section{Verificatio ex nihilo} \label{exnihilo}

\concept{Prove your template correct ``by fiat'': find out what properties and lemmas are required for each step of the proof to go through and assume they are part of your template.}

Mention functional induction here and how to derive it.

\section{Fill out the template -- concrete programs} \label{fillout}

\concept{Implement concrete versions of the algorithm by filling out the template with the required subprograms.}

\concept{Make sure the concrete algorithm is runnable without having to prove anything about it.}

Discussion about classes vs records vs modules, bundling and unbundling.

\section{Get your hands dirty -- concrete proofs}

\concept{Prove the concrete algorithm correct by filling out the proofs.}

\section{Summary}

\chapter{Conclusion}

Mention thesis' repository: \url{wkolowski.github.io/RandomCoqCode/Thesis/}

%%%%% BIBLIOGRAFIA

\begin{thebibliography}{1}

\bibitem{TheFreeDictionary}
    \url{https://www.thefreedictionary.com/algorithm}

\bibitem{SO}
    \textit{Do ``algorithms'' exist in Functional Programming?}, \\
    \url{https://stackoverflow.com/questions/25940327/do-algorithms-exist-in-functional-programming}

\bibitem{CLRS}
    Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein, \\
    \textit{Introduction to Algorithms}, \\
    \url{http://ressources.unisciel.fr/algoprog/s00aaroot/aa00module1/res/%5BCormen-AL2011%5DIntroduction_To_Algorithms-A3.pdf}

\bibitem{TAOCP}
    Donald Knuth, \textit{The Art of Computer Programming}

\bibitem{Kuhn}
    Thomas S. Kuhn, \textit{The Structure of Scientific Revolutions}

\bibitem{WordsMatter}
    Robert Harper, \textit{Words Matter}, 2012 \\
    \url{https://existentialtype.wordpress.com/2012/02/01/words-matter/}

\bibitem{PersistentDataStructures}
    Driscoll JR, Sarnak N, Sleator DD, Tarjan RE \\
    \textit{Making data structures persistent}, 1986

\bibitem{FunctionalUnionFind}
    Sylvain Conchon, Jean-Christophe Fillâtre, \\
    \textit{A Persistent Union-Find Data Structure}, 2007 \\
    \url{https://www.lri.fr/~filliatr/ftp/publis/puf-wml07.pdf}

\bibitem{Pippenger}
    Nicholas Pippenger, \textit{Pure versus impure Lisp}, 1996 \\
    \url{https://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf}

\bibitem{STMonad}
    John Launchbury, Simon Peyton Jones, \\
    \textit{Lazy Functional State Threads}, 1994 \\
    \url{https://www.microsoft.com/en-us/research/wp-content/uploads/1994/06/lazy-functional-state-threads.pdf}

\bibitem{ProofByContradiction}
    Andrej Bauer, \textit{Proof of negation and proof by contradiction}, 2010 \\
    \url{http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/}

\bibitem{Coq}
    \url{https://coq.inria.fr/}

\bibitem{CurryHoward}
    Morten Heine Sørensen, Paweł Urzyczyn, \\
    \textit{Lectures on the Curry-Howard Isomorphism}, 2006, \\
    \url{http://disi.unitn.it/~bernardi/RSISE11/Papers/curry-howard.pdf}

\bibitem{CoqInCoq}
    Bruno Barras, Benjamin Werner, \textit{Coq in Coq}, 1997, \\
    \url{http://www.lix.polytechnique.fr/Labo/Bruno.Barras/publi/coqincoq.pdf}

\bibitem{CoqCoqCorrect}
    Matthieu Sozeau, Simon Boulier, Yannick Forster, Nicolas Tabareau, Théo Winterhalter, \\
    \textit{Coq Coq Correct! Verification of Type Checking and Erasure for Coq, in Coq}, 2020, \\
    \url{https://www.irif.fr/~sozeau/research/publications/drafts/Coq_Coq_Correct.pdf}

\bibitem{Turing}
    Alan Turing, \textit{On Computable Numbers, with an Application to the Entscheidungsproblem}, 1936 \\
    \url{https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf}

\bibitem{Church}
    Alonzo Church, \textit{An Unsolvable Problem of Elementary Number Theory}, 1936 \\
    \url{https://www.ics.uci.edu/~lopes/teaching/inf212W12/readings/church.pdf}

\bibitem{OtherTuringMachine}
    Guy Blelloch, Robert Harper, \\
    \textit{$\lambda$-Calculus: The Other Turing Machine}, 2015 \\
    \url{https://www.cs.cmu.edu/~rwh/papers/lctotm/cs50.pdf}

\bibitem{ReasonableMachine}
    Ugo Dal Lago, Simone Martini, \\
    \textit{The Weak Lambda Calculus
    as a Reasonable Machine}, 2008 \\
    \url{https://www.di.unito.it/~deligu/CDR60_TCS/Martini.pdf}

\bibitem{ICC1}
    Ugo Dal Lago, \\
    \textit{A Short Introduction
    to Implicit Computational Complexity}, 2010 \\
    \url{http://cs.unibo.it/~dallago/FICQRA/esslli.pdf}

\bibitem{ICC2}
    Ugo Dal Lago,
    \textit{Machine-Free Complexity},
    2019 \\
    \url{https://caleidoscope.sciencesconf.org/data/DalLago_caleidoscopeslides.pdf}

\bibitem{CostSemanticsBlog}
    Robert Harper, \textit{Languages and Machines}, 2015 \\
    \url{https://existentialtype.wordpress.com/2011/03/16/languages-and-machines/}

\bibitem{CostSemantics}
    Norman Danner, Daniel R. Licata, Ramyaa Ramyaa \\
    \textit{Denotational Cost Semantics for
    Functional Languages with Inductive Types} \\
    \url{https://dlicata.wescreates.wesleyan.edu/pubs/dlr15inductive/dlr15inductive.pdf}

\bibitem{CoC}
    Thierry Coquand, Gérard Huet, \\
    \textit{The calculus of constructions}, 1984, \\
    \url{https://www.sciencedirect.com/science/article/pii/0890540188900053}

\bibitem{CIC}
    Christine Paulin-Mohring, \\
    \textit{Introduction to the Calculus of Inductive Constructions}, 2015 \\
    \url{https://hal.inria.fr/hal-01094195/document}

\bibitem{pCuIC}
    Amin Timany, Matthieu Sozeau, \\
    \textit{Consistency of the Predicative Calculus of Cumulative
    Inductive Constructions (pCuIC)}, 2018 \\
    \url{https://hal.inria.fr/hal-01615123v2/document}

\bibitem{MLTT1}
    Per Martin-L\"{o}f, \\
    \textit{An intuitionistic theory of types}, 1972 \\
    \url{https://archive-pml.github.io/martin-lof/pdfs/An-Intuitionistic-Theory-of-Types-1972.pdf}

\bibitem{MLTT2}
    Per Martin-L\"{o}f, \\
    \textit{Intuitionistic Type Theory}, 1984 \\
    \url{https://archive-pml.github.io/martin-lof/pdfs/Bibliopolis-Book-retypeset-1984.pdf}
    
\bibitem{SoftwareFoundations}
    Benjamin C. Pierce, Andrew W. Appel et al., \\
    \textit{Software Foundations}, 2019, \\
    \url{https://softwarefoundations.cis.upenn.edu/}

\bibitem{CoqArt}
    Yves Bertot and Pierre Castéran, \\
    \textit{Interactive Theorem Proving and Program Development \\ Coq'Art: The Calculus of Inductive Constructions}, 2004, \\
    \url{https://www.labri.fr/perso/casteran/CoqArt/}

\bibitem{CPDT}
    Adam Chlipala,
    \textit{Certified Programming with Dependent Types}, \\
    \url{http://adam.chlipala.net/cpdt/}

\bibitem{Okasaki}
    Chris Okasaki,
    \textit{Purely Functional Data Structures}, 1998

\bibitem{OkasakiPhD}
    Chris Okasaki,
    \textit{Purely Functional Data Structures} (PhD thesis), 1996 \\
    \url{https://www.cs.cmu.edu/~rwh/theses/okasaki.pdf}

\bibitem{Okasaki10YearsLater}
    Chris Okasaki,
    \textit{Ten Years of Purely Functional Data Structures}, 2008 \\
    \url{https://okasaki.blogspot.com/2008/02/ten-years-of-purely-functional-data.html}

\bibitem{SinceOkasaki}
    \textit{What's new in purely functional data structures since Okasaki?}, \\
    \url{https://cstheory.stackexchange.com/questions/1539/whats-new-in-purely-functional-data-structures-since-okasaki}

\bibitem{VFA}
    Andrew W. Appel,
    \textit{Verified Functional Algorithms}, 2018 \\
    \url{https://softwarefoundations.cis.upenn.edu/vfa-current/index.html}
\end{thebibliography}

\end{document}