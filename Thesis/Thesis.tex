% Opcje klasy 'iithesis' opisane sa w komentarzach w pliku klasy. Za ich pomoca
% ustawia sie przede wszystkim jezyk i rodzaj (lic/inz/mgr) pracy, oraz czy na
% drugiej stronie pracy ma byc skladany wzor oswiadczenia o autorskim wykonaniu.
\documentclass[declaration,mgr,english,shortabstract]{iithesis}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}

%%%%% DANE DO STRONY TYTULOWEJ
% Niezaleznie od jezyka pracy wybranego w opcjach klasy, tytul i streszczenie
% pracy nalezy podac zarowno w jezyku polskim, jak i angielskim.
% Pamietaj o madrym (zgodnym z logicznym rozbiorem zdania oraz estetyka) recznym
% zlamaniu wierszy w temacie pracy, zwlaszcza tego w jezyku pracy. Uzyj do tego
% polecenia \fmlinebreak.
\englishtitle   {Formally verified algorithms and data structures in Coq: concepts and techniques}
\polishtitle    {Formalnie zweryfikowane algorytmy i struktury danych w Coqu: koncepcje i techniki}
\polishabstract {Omawiamy sposoby projektowania, implementowania, specyfikowania i weryfikowania funkcyjnych algorytmów i struktur danych, skupiając się bardziej na dowodach formalnych niż na asymptotycznej złożoności czy faktycznym czasie działania. Prezentujemy koncepty i techniki, obie często opierające na jednej kluczowej zasadzie -- reifikacji i reprezentacji, za pomocą potężnego systemu typów Coqa, czegoś co w klasycznym, imperatywnym podejściu jest nieuchwytne, jak przepływ informacji w dowodzie czy kształt rekursji funkcji. Nasze podejście bogato ilustrujemy przykładami i studiami przypadku.}
\englishabstract{We discuss how to design, implement, specify and verify functional algorithms and data structures, concentrating on formal proofs rather than asymptotic complexity or actual performance. We present concepts and techniques, both of which often rely on one key principle -- the reification and representation, using Coq's powerful type system, of something which in the classical-imperative approach is intangible, like the flow of information in a proof or the shape of a function's recursion. We illustrate our approach using rich examples and case studies.}
% w pracach wielu autorow nazwiska mozna oddzielic poleceniem \and
\author         {Wojciech Kołowski}
% w przypadku kilku promotorow, lub koniecznosci podania ich afiliacji, linie
% w ponizszym poleceniu mozna zlamac poleceniem \fmlinebreak
\advisor        {oficjalnienarazienikt}
\date           {Czerwiec '20 chyba że koronawirus}                     % Data zlozenia pracy
% Dane do oswiadczenia o autorskim wykonaniu
%\transcriptnum {}                     % Numer indeksu
%\advisorgen    {dr. Jana Kowalskiego} % Nazwisko promotora w dopelniaczu
%%%%%

%%%%% WLASNE DODATKOWE PAKIETY
%
%\usepackage{graphicx,listings,amsmath,amssymb,amsthm,amsfonts,tikz}
\usepackage{minted}
%
%%%%% WLASNE DEFINICJE I POLECENIA
%
%\theoremstyle{definition} \newtheorem{definition}{Definition}[chapter]
%\theoremstyle{remark} \newtheorem{remark}[definition]{Observation}
%\theoremstyle{plain} \newtheorem{theorem}[definition]{Theorem}
%\theoremstyle{plain} \newtheorem{lemma}[definition]{Lemma}
%\renewcommand \qedsymbol {\ensuremath{\square}}

\newcommand{\m}[1]{\texttt{#1}}

%%%%%

\begin{document}

%%%%% POCZATEK ZASADNICZEGO TEKSTU PRACY

\chapter{Introduction} \label{ch1}

The title of this thesis is ``Formally verified functional algorithms and data structures in Coq: concepts and techniques''. In the Introduction, we take time to carefully explain what we mean by each of these words:

\begin{itemize}
    \item In section \ref{ch1s1} we look at the social context that gives rise to an interesting misconception about ``algorithms and data structures''.
    \item In section \ref{ch1s2} we explain ``functional'' programming by contrasting it with imperative programming.
    \item In section \ref{ch1s3} we lay out the unity of the ``formally verified'' world by comparing it with the many worlds which are the arenas of the usual algorithmic struggles.
    \item In section \ref{ch1s4} we do an ultra short literature review and discuss how our ``concepts and techniques'' differ from these that can be found there.
    \item In section \ref{ch1s5} we show how working ``in Coq'' looks like and describe some theory and principles Coq is based on.
\end{itemize}

In the remaining chpaters we describe the actual concepts and techniques, using well-known and well-studied examples like quicksort, merge sort or binary heaps:

\begin{itemize}
    \item TODO
\end{itemize}

\section{An overarching paradigm} \label{ch1s1}

The Free Dictionary says \cite{TheFreeDictionary} that an algorithm is

\begin{quote}
    A finite set of unambiguous instructions that, given some set of initial conditions, can be performed in a prescribed sequence to achieve a certain goal and that has a recognizable set of end conditions.
\end{quote}

The purpose of this entry is to explain the concept to a lay person, but it likely sounds just about right to the imperative programmer's ear too. To a functional ear, however, talking about sequences of instructions most certainly sounds as un-functional as it possibly could. It is no surprise then that some people wonder if it is even possible for algorithms to ``exist'' in a functional programming language, as exemplified by this StackOverflow question \cite{SO}. The poor soul asking this question had strongly associated algorithms with imperative languages in his head, even though functional languages have their roots in lambda calculus, a formal system invented precisely to capture what an algorithm is.

This situation is not uncommon and rather easy to explain. \textit{Imperative} algorithms and data structures \footnote{From now on when we write ``algorithms'' we will mean ``algorithms and data structures''} form one of the oldest, biggest, most widespread and prestigious fields of theoretical computer science. They are taught to every student in every computer science programme at every university. There's a huge amount of books and textbooks, with classics such as \cite{CLRS} \cite{TAOCP} known to pretty much everybody, at least by title. There's an even huger and still growing mass of research articles published in journals and conferences and I'm pretty sure there are at least some (imperative) algorithm researchers at every computer science department in existence.

But theoretical research and higher education are not the only strongholds of imperative algorithms. They are also pretty much synonymous with competitive programming, dominating most in-person and online computer science competitions like ICPC and HackerRank, respectively. They are seen as the thing that gifted high school students interested in computer science should pursue -- each time said students don't win medals in the International Olympiad in Informatics or the like, there will be some journalists complaining that their country's education system is ``falling behind''. In many countries, like Poland,\footnote{I believe the same is true for the rest of the former Eastern Bloc countries too and probably not much better in the West either.} if there's any high school level computer science education besides basic programming, it will be in imperative algorithms.

Imperative algorithms aren't just a mere field of study -- they are more of a mindset and a culture; following Kuhn's \cite{Kuhn} terminology, they can be said to form a paradigm. Because this paradigm is so immense, so powerful and so entrenched, we feel free to completely disregard it and devote ourselves and this thesis to studying a related field which did not yet reach the status of a paradigm -- functional algorithms -- focusing on proving their formal correctness.

But before we do that, we spend the rest of this chapter comparing the imperative and functional approaches to algorithms and briefly reviewing available literature on functional algorithms.

\section{Two flavours of algorithms} \label{ch1s2}

The differences between the fields imperative and functional algorithms are mostly a reflection of the differences between imperative and functional programming languages and only in a small part a matter of differences in research focus.

The basic data structure in imperative languages is the array, which abstracts a contiguous block of memory holding values of a particular type. More advanced data structures are usually records that hold values and pointers/references to other (or even the same) kinds of data structures. The basic control flow primitives for traversing these structures are various loops (\m{while}, \m{for}, \m{do} ... \m{while}) and branch/jump statements (\m{if}, \m{switch}, \m{goto}). The most important operation is assignment, which changes the value of a variable (and thus variables do actually vary, like the name suggests \cite{WordsMatter}). Computation is modeled by a series of operations which change the global state.

In functional languages the basic data structures are created using the mechanism of algebraic data types -- elements of each type so defined are trees whose node labels, branching and types of values held are specified by the user. The basic control flow primitives are pattern matching (checking the label and values of the tree's root) and recursion. The most important operation is function composition, which allows building complex functions from simpler ones. Computation is modeled by substitution of arguments for the formal parameters of a function. Variables don't actually vary -- they are just names for expressions. \cite{WordsMatter}

\begin{listing}[H]
    \begin{minted}{Java}
        int sum(int[] a)
        {
            int result = 0;
            for(int i = 0; i < a.length; ++i)
            {
                result += a[i];
            }
            return result;
        }
    \end{minted}
    \caption{A simple program for summing all integers stored in an array, written in an imperative pseudocode that resembles Java.}
\end{listing}

\begin{listing}[H]
    \begin{minted}{Haskell}
        data List a = Nil | Cons a (List a)

        sum : List Int -> Int
        sum Nil = 0
        sum (Cons x xs) = x + sum xs
    \end{minted}
    \begin{center}
        \caption{A simple program for summing all integers stored in a (singly-linked) list, written in a functional pseudocode that resembles Haskell.}
    \end{center}
\end{listing}

The two above programs showcase the relevant differences in practice. In both cases we want a function that sums integers stored in the most basic data structure. In the case of our pseudo-Java, this is a built-in array, whereas in our pseudo-Haskell, this is a list defined using the mechanism of algebraic data types, as something which is either empty (\m{Nil}) or something that has a head of type \m{a} and a tail, which is another list of values of type \m{a}.

In the imperative program, we declare a variable \m{result} to hold the current value of the sum and then we loop over the array. We start by creating an iterator variable \m{i} and setting it to \m{0}. We successively increment it with each iteration of the loop until it gets bigger than the length of the array and the loop finishes. At each iteration, we modify \m{result} by adding to it the array entry we're currently looking at.

In the functional program, we pattern match on the argument of the function. In case it is \m{Nil} (an empty list), we declare the result to be \m{0}. In case it is \m{Cons x xs} (a list with head \m{x} and tail \m{xs}), we declare that the result is computed by adding \m{x} and the recursively computed sum of numbers from the list \m{xs}.

Even though these programs are very simple, they depict the basic differences between imperative and functional algorithms quite well. Some less obvious differences are as follows.

First, functional data structures are by default immutable and thus persistent \cite{PersistentDataStructures}, whereas this is not the case for imperative data structures -- they have to be explicitly designed to support persistence. This means implementing some techniques, like backtracking, is very easy in functional languages, but it often requires much more effort in imperative languages. The price of persistence often is, however, increased memory usage.

Second, pointer juggling in imperative languages allows a more efficient implementation of some operations on tree-like structures than using algebraic data types, because nodes in such trees can have pointers not only to their children, but also to parents, siblings, etc. The most famous data structure whose functional, asymptotically optimal implementation is not known is union-find. \cite{FunctionalUnionFind}

The third point is that arrays, the bread-and-butter of imperative programming, provide random access read and write in $O(1)$ time, whereas equivalent functional random access structures work in $O(\log n)$ time where $n$ is the array size (or, at best, $O(\log i)$ where $i$ is the accessed index). This means that algorithms relying on constant time random access will suffer an asymptotic performance penalty when implemented in functional languages. \cite{Pippenger}

Even though this asymptotic penalty sounds gloomy, not all hope is lost, because of two reasons. First, mutable arrays can still be used in purely\footnote{``Pure'' and ``impure'' are loose terms used to classify functional languages. ``Pure'' roughly means that a language makes organized effort to separate programs that do ordinary number crunching or data processing from those that have side effects, like throwing exceptions or connecting to a database. An ``impure'' languages doesn't attempt at such a separation.} functional languages if the mutability is hidden behind a pure interface. An example of this is Haskell's ST monad. \cite{STMonad} Second, functional languages that are impure, like Standard ML or OCaml, allow using mutable arrays without much hassle, which often saves the day.

\section{The many-worlds interpretation of the algorithmic business} \label{ch1s3}

We stated earlier that we intend to concentrate on formally proving correctness of purely functional algorithms. Before doing that, we shortly peek at how it differs from the activities and workflows of the usual algorithmic business and what we're leaving out.

In the following paragraphs, we take the perspective of a person we will call, for the lack of a better English word, an \textit{algorithmist}, by which we mean someone who specifies, designs and analyzes (and also maybe implements, tests, profiles, etc.) algorithms.

``Necessity is the mother of invention'', goes the proverb, and indeed most algorithms arise as answers to questions like ``How do I make a computer sort a list of book titles?'' or ``How do I arrange my files on the hard disk so that I can easily find them later?''. The path toward this answer looks somewhat like this:

\begin{itemize}
    \item Provide a more precise specification of the problem. For example: ``Given an array $A$ of length $n$ which contains integers, rearrange them so that for every index $i < n$ we have $A[i] \leq A[i + 1]$''
    \item Design an algorithm that solves the problem.
    \item Prove that the algorithm is correct. If a proof is hard to find, this may be the sign that the algorithm is incorrect.
    \item Analyze complexity of the algorithm, i.e. how much resources (number of steps, bits memory, bits of randomness, etc.) does the algorithm need to solve the problem of a given size. If the algorithm needs too much resources, go back and try to design another one that needs less.
    \item Implement the algorithm and test whether the implementation is correct.
    \item Run some experiments to assess the actual performance of the implementation in a few scenarios: random data, data that often occurs in the real world, etc. If the performance is bad, try to find a better implementation or go back and design another algorithm.
\end{itemize}

An enlightening thing to do at this point is notice all the different worlds in which these activites take place. The specification is usually written in English (or whatever the language of the algorithmist), intermingled with some mathematical symbols like quantifiers. The algorithm design process takes place in the algorithmist's mind, but after he's done, it is usually written down in some kind of comfortable pseudocode that allows skipping inessential detail.

The proof is again written in a mixture of English and mathematical symbols, but that's only the surface view. If we take a closer look, it will most likely turn out that the mathematical part is based on classical logic (which admits reasoning principles like proof by contradiction \cite{ProofByContradiction} - this can be often seen in proofs of optimality) and some kind of naive set theory, but if pressed a bit, the algorithmist would readily assert that the set theory could be axiomatized using, for example, the Zermelo-Fraenkel axioms.

The next hidden world, in which the complexity analysis takes its place, is the model of computation. Most likely it won't be mentioned explicitly, just like the logic and set theory thing in the previous paragraph, but we can easily guess it from the fact that the algorithm is intended to be implemented on a real computer and the model of computation most similar to the workings of real hardware is the RAM machine. Last but not least, the implementation, in contrast to the algorithm's pseudocode, will be in some concrete programming language - a very popular algorithmic language, due to its speed, is \m{C++}.

As we see, a typical algorithmic solution of a problem stems from a sequence (or rather, a loop) of activities which live in quite different worlds. Even though in practice the above recipe works well A person concerned with the formal correctness of such a solution could have many doubts related to how to worlds fit together:

\begin{itemize}
    \item Does the pseudocode really describe an algorithm that meets the informally written specification? Is the pen-and-paper proof correct?
    \item Does the implementation match the pseudocode? Do tests match the specification?
    \item Does the assumed model of computation match the capabilities of the programming language used for implementation? Is the analysis correct? Do its results match the assessed performance?
    \item 
\end{itemize}



\section{An overview of available literature} \label{ch1s4}

Literature review, Okasaki is old and bad for Coq, SF3 is shallow.

\section{Dupa konia na wesoło} \label{ch1s5}

\chapter{Quicksort - \textit{tour de force}} \label{ch2}


\chapter{A man, a plan, a canal -- MSc thesis}

\section{Design}
\begin{itemize}
    \item First step: specification (describe the path from intuition to formal spec).
    \item Second step: design (describe the path from the concrete to the abstract by tracing a stub proof of correctness).
    \item We shouldn't require proofs in order to run programs. Clou: packed vs unpacked classes/records/modules.
\end{itemize}

\section{Techniques}
\begin{itemize}
    \item General recursion: Bove-Capretta method as the way to go.
    \item Functional induction as the way-to-go proof technique. Mention the Equations plugin.
\end{itemize}

\section{Topics}
\begin{itemize}
    \item Quicksort: in functional languages we have so powerful abstractions that we can actually implement \*algorithms\* and not just programs.
    \item  Braun mergesort: in order not to waste resources, we sometimes have to reify abstract patterns, like the splitting in mergesort.
    \item Binary heaps: a case study to show the basic workflow and that it's not that obvious how to get basic stuff right.
    \item Cool data structures: ternary search trees, finger trees.
\end{itemize}


%%%%% BIBLIOGRAFIA

\begin{thebibliography}{1}

\bibitem{TheFreeDictionary}
    \url{https://www.thefreedictionary.com/algorithm}

\bibitem{SO}
    \textit{Do ``algorithms'' exist in Functional Programming?}, \\
    \url{https://stackoverflow.com/questions/25940327/do-algorithms-exist-in-functional-programming}

\bibitem{CLRS}
    Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein, \\
    \textit{Introduction to Algorithms}, \\
    \url{http://ressources.unisciel.fr/algoprog/s00aaroot/aa00module1/res/%5BCormen-AL2011%5DIntroduction_To_Algorithms-A3.pdf}

\bibitem{TAOCP}
    Donald Knuth, \textit{The Art of Computer Programming}

\bibitem{Kuhn}
    Thomas S. Kuhn, \textit{The Structure of Scientific Revolutions}

\bibitem{WordsMatter}
    Robert Harper, \textit{Words Matter}, 2012 \\
    \url{https://existentialtype.wordpress.com/2012/02/01/words-matter/}

\bibitem{PersistentDataStructures}
    Driscoll JR, Sarnak N, Sleator DD, Tarjan RE \\
    \textit{Making data structures persistent}, 1986

\bibitem{FunctionalUnionFind}
    Sylvain Conchon, Jean-Christophe Fillâtre, \\
    \textit{A Persistent Union-Find Data Structure}, 2007 \\
    \url{https://www.lri.fr/~filliatr/ftp/publis/puf-wml07.pdf}

\bibitem{Pippenger}
    Nicholas Pippenger, \textit{Pure versus impure Lisp}, 1996 \\
    \url{https://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Pure%20Versus%20Impure%20LISP.pdf}

\bibitem{STMonad}
    John Launchbury, Simon Peyton Jones, \\
    \textit{Lazy Functional State Threads}, 1994 \\
    \url{https://www.microsoft.com/en-us/research/wp-content/uploads/1994/06/lazy-functional-state-threads.pdf}

\bibitem{ProofByContradiction}
    Andrej Bauer, \textit{Proof of negation and proof by contradiction}, 2010 \\
    \url{http://math.andrej.com/2010/03/29/proof-of-negation-and-proof-by-contradiction/}
\end{thebibliography}

\end{document}